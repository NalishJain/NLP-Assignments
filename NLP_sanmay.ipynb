{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "X-zbEjatyR54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)"
      ],
      "metadata": {
        "id": "zon-aEEKmHbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLM_efficient:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 0\n",
        "        self.vocabulary_index = {}\n",
        "        self.word_count = {}\n",
        "        self.index_vocabulary = {}\n",
        "        self.bigram_counts = None\n",
        "        self.bigram_probabilities = None\n",
        "        self.dataset = None\n",
        "\n",
        "\n",
        "    def build_corpus(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            sentences = [line.strip().split() for line in file]\n",
        "        self.dataset =  sentences\n",
        "\n",
        "    def build_vocab(self):\n",
        "        for line in self.dataset:\n",
        "            for word in line:\n",
        "                if word not in self.vocabulary_index:\n",
        "                    self.vocabulary_index[word] = self.vocab_size\n",
        "                    self.index_vocabulary[self.vocab_size] = word\n",
        "                    self.word_count[word] = 0\n",
        "                    self.vocab_size += 1\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "    def build_probability_matrix(self, mode, discount=0, emotion_id=0):\n",
        "        self.bigram_probabilities = np.zeros((self.vocab_size, self.vocab_size), dtype=float)\n",
        "\n",
        "        if mode == 0:\n",
        "            self.bigram_probabilities = self.calculate_probability_matrix()\n",
        "        elif mode == 1:\n",
        "            self.bigram_probabilities = self.laplace_smoothing_matrix()\n",
        "        elif mode == 2:\n",
        "            self.bigram_probabilities = self.kneser_ney_smoothing_matrix(discount=discount)\n",
        "\n",
        "\n",
        "    def calculate_probability_matrix(self):\n",
        "        return self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
        "\n",
        "    def laplace_smoothing_matrix(self):\n",
        "        return (self.bigram_counts + 1) / (self.word_count_matrix[:, np.newaxis] + self.vocab_size)\n",
        "\n",
        "    def kneser_ney_smoothing_matrix(self, discount=0):\n",
        "        discounted_probs = np.maximum(self.bigram_counts - discount, 0) / self.word_count_matrix[:, np.newaxis]\n",
        "        alpha_word1 = (discount * np.sum(self.bigram_counts > 0, axis=1)) / self.word_count_matrix\n",
        "        cont_probs = np.sum(self.bigram_counts > 0, axis=0) / np.sum(self.bigram_counts > 0)\n",
        "        return discounted_probs + alpha_word1[:, np.newaxis] * cont_probs\n",
        "\n",
        "    def calculate_probability_emotion_row(self, first_wrod, emotion_id=0):\n",
        "        non_zero_indices = np.nonzero(self.bigram_counts[self.vocabulary_index[first_wrod], :])[0]\n",
        "        emo_probs = np.zeros((self.vocab_size, 6))\n",
        "        for second_word_index in non_zero_indices:\n",
        "            k = self.emotion_scores(first_wrod + \" \" +  self.index_vocabulary[second_word_index])\n",
        "            prob_score = []\n",
        "            for label_score in range(6):\n",
        "                prob_score.append(k[label_score]['score'])\n",
        "            emo_probs[second_word_index] = np.array(prob_score)\n",
        "        first_word_mat = self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
        "        return first_word_mat[self.vocabulary_index[first_wrod],:] + emo_probs[:, emotion_id]\n",
        "\n",
        "    def emotion_scores(self, sample):\n",
        "        emotion=classifier(sample)\n",
        "        return emotion[0]\n",
        "\n",
        "    def learn(self, file_path):\n",
        "        self.build_corpus(file_path)\n",
        "        self.build_vocab()\n",
        "\n",
        "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
        "        self.word_count_matrix = np.array(list(self.word_count.values()))\n",
        "\n",
        "        for line in self.dataset:\n",
        "            for index in range(len(line) - 1):\n",
        "                first_word_index = self.vocabulary_index[line[index]]\n",
        "                second_word_index = self.vocabulary_index[line[index + 1]]\n",
        "                self.bigram_counts[first_word_index, second_word_index] += 1\n",
        "\n",
        "    def generate_samples(self, emotion_id = 0, num_samples = 50):\n",
        "            generated_samples = []\n",
        "            for _ in range(num_samples):\n",
        "                sample = self.generate_sample(emotion_id)\n",
        "                generated_samples.append(sample)\n",
        "            return generated_samples\n",
        "\n",
        "    def generate_sample(self, emotion_id = 0, max_length = 10):\n",
        "\n",
        "        start_word = np.random.choice(['i', 'im', 'ive'], p = [0.7, 0.2, 0.1])\n",
        "        current_word = start_word\n",
        "        sample = [current_word]\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "\n",
        "            current_word_index = self.vocabulary_index[current_word]\n",
        "            probabilities = self.calculate_probability_emotion_row(current_word, emotion_id)\n",
        "\n",
        "            if np.all(probabilities == 0):\n",
        "                break\n",
        "\n",
        "            probabilities /= probabilities.sum()\n",
        "\n",
        "            next_word_index = np.random.choice(self.vocab_size, p = probabilities)\n",
        "            next_word = self.index_vocabulary[next_word_index]\n",
        "\n",
        "            sample.append(next_word)\n",
        "            current_word = next_word\n",
        "\n",
        "        return ' '.join(sample)\n",
        "\n",
        "\n",
        "    def find_top_bigrams(self, num_top_bigrams=5):\n",
        "        top_bigrams = []\n",
        "        bigram_prob_scores = []\n",
        "\n",
        "        for i in range(self.vocab_size):\n",
        "            for j in range(self.vocab_size):\n",
        "                bigram = f\"{self.index_vocabulary[i]} {self.index_vocabulary[j]}\"\n",
        "                bigram_prob = self.bigram_probabilities[i, j]\n",
        "                bigram_prob_scores.append((bigram, bigram_prob))\n",
        "\n",
        "        sorted_bigrams = sorted(bigram_prob_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        top_bigrams = sorted_bigrams[:num_top_bigrams]\n",
        "\n",
        "        return top_bigrams"
      ],
      "metadata": {
        "id": "KQMfLtThmhf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = 'corpus.txt'\n",
        "bigram_model = BigramLM_efficient()\n",
        "bigram_model.learn(corpus_path)"
      ],
      "metadata": {
        "id": "_ps5peL5DEBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.build_probability_matrix(0)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (Before smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfGfDDkTfs0",
        "outputId": "f0f389ca-2029-4923-a195-2ba31d0c42ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Bigrams (Before smoothing):\n",
            "\n",
            "Bigram: 'href http',   Probability: 1.0000\n",
            "Bigram: 'tychelle to',   Probability: 1.0000\n",
            "Bigram: 'hang out',   Probability: 1.0000\n",
            "Bigram: 'nonexistent social',   Probability: 1.0000\n",
            "Bigram: 'alex and',   Probability: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.build_probability_matrix(1)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (After Laplace smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvjCbPGYUByB",
        "outputId": "3254800b-0fbc-4bdb-9dbe-20b5e45e6fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Bigrams (After Laplace smoothing):\n",
            "\n",
            "Bigram: 'i feel',   Probability: 0.1104\n",
            "Bigram: 'feel like',   Probability: 0.0351\n",
            "Bigram: 'i am',   Probability: 0.0319\n",
            "Bigram: 'that i',   Probability: 0.0265\n",
            "Bigram: 'and i',   Probability: 0.0231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.build_probability_matrix(2, 0.5)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (After Kneser Ney smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3RPAfebUB4f",
        "outputId": "0c11df05-dc5e-431d-caf2-43aba75e01af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Bigrams (After Kneser Ney smoothing):\n",
            "\n",
            "Bigram: 'href http',   Probability: 0.9800\n",
            "Bigram: 'don t',   Probability: 0.9746\n",
            "Bigram: 'didn t',   Probability: 0.9722\n",
            "Bigram: 'sort of',   Probability: 0.9710\n",
            "Bigram: 'supposed to',   Probability: 0.9456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "for emotion in range(len(emotions)):\n",
        "    generated_samples = bigram_model.generate_samples(num_samples = 50, emotion_id=emotion)\n",
        "    output_file = f'gen_{emotions[emotion]}.txt'\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for sample in generated_samples:\n",
        "            file.write(sample + '\\n')"
      ],
      "metadata": {
        "id": "_z3iN8s4mLhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = 'corpus.txt'\n",
        "labels_path = 'labels.txt'\n",
        "\n",
        "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
        "    texts = [line.strip() for line in file]\n",
        "\n",
        "with open(labels_path, 'r', encoding='utf-8') as file:\n",
        "    labels = [line.strip() for line in file]\n",
        "\n",
        "\n",
        "X_train, y_train = texts, labels\n",
        "\n",
        "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "testing_samples = []\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_file_path = f'gen_{emotion}.txt'\n",
        "    with open(emotion_file_path, 'r', encoding='utf-8') as file:\n",
        "        emotion_samples = [line.strip() for line in file]\n",
        "\n",
        "        X_test.extend(emotion_samples)\n",
        "        y_test.extend([emotion] * len(emotion_samples))\n",
        "\n",
        "y_test = list(y_test)"
      ],
      "metadata": {
        "id": "TLrW1N1n7mk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "svc_model = SVC(kernel = 'linear', C = 120, gamma = 0.002, break_ties = True, probability = True)\n",
        "svc_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = svc_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOW_BFWuerHT",
        "outputId": "f388ed6a-4504-4613-9c03-90aca4d29126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 74.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': [100, 115, 125],\n",
        "    'gamma': [0.001, 0.007, 0.0096],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "svc_model = SVC(break_ties = True, probability = True)\n",
        "\n",
        "grid_search = GridSearchCV(estimator = svc_model, param_grid = param_grid, cv = 5, scoring = 'accuracy')\n",
        "\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred_grid = best_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy_grid = accuracy_score(y_test, y_pred_grid)\n",
        "classification_rep = classification_report(y_test, y_pred_grid)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy with Grid Search:\", accuracy_grid * 100)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY3rrDnCJvgb",
        "outputId": "6fe845aa-0d0e-4d64-f5eb-1fdcb207507a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 115, 'gamma': 0.0096, 'kernel': 'rbf'}\n",
            "Accuracy with Grid Search: 75.33333333333333\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.42      0.56        50\n",
            "        fear       0.89      0.68      0.77        50\n",
            "         joy       0.59      0.68      0.63        50\n",
            "        love       0.85      0.90      0.87        50\n",
            "     sadness       0.59      0.86      0.70        50\n",
            "    surprise       0.92      0.98      0.95        50\n",
            "\n",
            "    accuracy                           0.75       300\n",
            "   macro avg       0.78      0.75      0.75       300\n",
            "weighted avg       0.78      0.75      0.75       300\n",
            "\n"
          ]
        }
      ]
    }
  ]
}