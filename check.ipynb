{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at bhadresh-savani/distilbert-base-uncased-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np \n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scores(sample): \n",
    "    emotion=classifier(sample)\n",
    "    return emotion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i crashed and heading 0.8968580961227417\n",
      "i often pass on 0.02199549227952957\n",
      "i awoke an ebayed 0.9508686661720276\n",
      "i seemed so vulnerable 0.9965572357177734\n",
      "i live in bellingham 0.5385473370552063\n",
      "i suspect professional pressures 0.06531106680631638\n",
      "i expected the netherlands 0.13407382369041443\n",
      "i suspect feel frightened 0.997471809387207\n",
      "i might push through 0.3585923910140991\n",
      "i could loose my 0.014337671920657158\n"
     ]
    }
   ],
   "source": [
    "with open('gen_3_fear.txt', 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.strip() for line in file]\n",
    "\n",
    "# Count the number of samples with a score greater than 0.8\n",
    "count_high_score = 0\n",
    "i = 0\n",
    "indices = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    scores = emotion_scores([sentence])\n",
    "    max_score = scores[4]['score']\n",
    "    print(sentence, max_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.010323652066290379},\n",
       " {'label': 'joy', 'score': 0.022301768884062767},\n",
       " {'label': 'love', 'score': 0.9226101636886597},\n",
       " {'label': 'anger', 'score': 0.02163187973201275},\n",
       " {'label': 'fear', 'score': 0.01970757357776165},\n",
       " {'label': 'surprise', 'score': 0.003424924099817872}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_scores(\"im craving theres any other\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
