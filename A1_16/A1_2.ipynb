{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X-zbEjatyR54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: numpy in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (1.26.3)\n",
            "Requirement already satisfied: tensorflow in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
            "Requirement already satisfied: setuptools in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zon-aEEKmHbv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at bhadresh-savani/distilbert-base-uncased-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class BigramLM:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 0\n",
        "        self.vocabulary_index = {}\n",
        "        self.word_count = {}\n",
        "        self.index_vocabulary = {}\n",
        "        self.bigram_counts = None\n",
        "        self.bigram_probabilities = None\n",
        "        self.dataset = None\n",
        "\n",
        "\n",
        "    def build_corpus(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            sentences = [line.strip().split() for line in file]\n",
        "        self.dataset =  sentences\n",
        "    \n",
        "    def build_vocab(self):\n",
        "        for line in self.dataset:\n",
        "            for word in line:\n",
        "                if word not in self.vocabulary_index:\n",
        "                    self.vocabulary_index[word] = self.vocab_size\n",
        "                    self.index_vocabulary[self.vocab_size] = word\n",
        "                    self.word_count[word] = 0\n",
        "                    self.vocab_size += 1\n",
        "                self.word_count[word] += 1\n",
        "    def learn(self, file_path):\n",
        "        # Build vocabulary and initialize bigram counts\n",
        "        self.build_corpus(file_path)\n",
        "        self.build_vocab()\n",
        "\n",
        "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
        "\n",
        "        for line in self.dataset:\n",
        "            for index in range(len(line) - 1):\n",
        "                first_word_index = self.vocabulary_index[line[index]]\n",
        "                second_word_index = self.vocabulary_index[line[index + 1]]\n",
        "                self.bigram_counts[first_word_index, second_word_index] += 1\n",
        "\n",
        "    def calculate_probability(self, word1, word2):\n",
        "        return self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]/self.word_count[word1]\n",
        "    \n",
        "    def laplace_smoothing(self, word1, word2):\n",
        "        return (self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]] + 1)/(self.word_count[word1] + self.vocab_size)\n",
        "\n",
        "    def kneser_ney_smoothing(self, word1, word2, discount = 0):\n",
        "        discounted_prob = max(self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]-discount, 0)/self.word_count[word1]\n",
        "        alpha_word1 = (discount* np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))/self.word_count[word1]\n",
        "        cont_prob = np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0)/np.sum(self.bigram_counts > 0)\n",
        "        print(\"alpha_word1\", alpha_word1)\n",
        "        print(\"discounted_prob\",discounted_prob )\n",
        "        print(\"self.word_count[word1]\", self.word_count[word1])\n",
        "        print(\"np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))\", np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))\n",
        "        print(\"np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0)\", np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0))\n",
        "        print(\"np.sum(self.bigram_counts > 0)\", np.sum(self.bigram_counts > 0))\n",
        "\n",
        "        return discounted_prob + alpha_word1*cont_prob\n",
        "    \n",
        "    def emotion_scores(self, sample): \n",
        "        emotion=classifier(sample)\n",
        "        return emotion[0]\n",
        "\n",
        "    def calculate_probability_emotion(self, word1, word2, emotion_id):\n",
        "        emo_prob = self.emotion_scores(word1 + \" \" +word2)\n",
        "        return self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]/self.word_count[word1] + emo_prob[emotion_id]['score']\n",
        "    \n",
        "    def build_probability_matrix(self, mode, discount = 0, emotion_id = 0):\n",
        "        self.bigram_probabilities =  np.zeros((self.vocab_size, self.vocab_size), dtype=float)\n",
        "        if mode == 0:\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.vocab_size):\n",
        "                    self.bigram_probabilities[i, j] =  self.calculate_probability(self.index_vocabulary[i], self.index_vocabulary[j])\n",
        "\n",
        "        elif mode == 1:\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.vocab_size):\n",
        "                    self.bigram_probabilities[i, j] =  self.laplace_smoothing(self.index_vocabulary[i], self.index_vocabulary[j])\n",
        "                    \n",
        "        elif mode == 2:\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.vocab_size):\n",
        "                    print(i, j)\n",
        "                    self.bigram_probabilities[i, j] =  self.kneser_ney_smoothing(self.index_vocabulary[i], self.index_vocabulary[j], discount= discount) \n",
        "\n",
        "        else:\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.vocab_size):\n",
        "                    print(i, j)\n",
        "                    self.bigram_probabilities[i, j] =  self.calculate_probability_emotion(self.index_vocabulary[i], self.index_vocabulary[j], emotion_id= emotion_id)                              \n",
        "# Display the formed corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KQMfLtThmhf1"
      },
      "outputs": [],
      "source": [
        "class BigramLM_efficient:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 0\n",
        "        self.vocabulary_index = {}\n",
        "        self.word_count = {}\n",
        "        self.index_vocabulary = {}\n",
        "        self.bigram_counts = None\n",
        "        self.bigram_probabilities = None\n",
        "        self.dataset = None\n",
        "\n",
        "\n",
        "    def build_corpus(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            sentences = [line.strip().split() for line in file]\n",
        "        self.dataset =  sentences\n",
        "\n",
        "    def build_vocab(self):\n",
        "        for line in self.dataset:\n",
        "            for word in line:\n",
        "                if word not in self.vocabulary_index:\n",
        "                    self.vocabulary_index[word] = self.vocab_size\n",
        "                    self.index_vocabulary[self.vocab_size] = word\n",
        "                    self.word_count[word] = 0\n",
        "                    self.vocab_size += 1\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "    def build_probability_matrix(self, mode, discount=0, emotion_id=0):\n",
        "        self.bigram_probabilities = np.zeros((self.vocab_size, self.vocab_size), dtype=float)\n",
        "\n",
        "        if mode == 0:\n",
        "            self.bigram_probabilities = self.calculate_probability_matrix()\n",
        "        elif mode == 1:\n",
        "            self.bigram_probabilities = self.laplace_smoothing_matrix()\n",
        "        elif mode == 2:\n",
        "            self.bigram_probabilities = self.kneser_ney_smoothing_matrix(discount=discount)\n",
        "\n",
        "\n",
        "    def calculate_probability_matrix(self):\n",
        "        return self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
        "\n",
        "    def laplace_smoothing_matrix(self):\n",
        "        return (self.bigram_counts + 1) / (self.word_count_matrix[:, np.newaxis] + self.vocab_size)\n",
        "\n",
        "    def kneser_ney_smoothing_matrix(self, discount=0):\n",
        "        discounted_probs = np.maximum(self.bigram_counts - discount, 0) / self.word_count_matrix[:, np.newaxis]\n",
        "        alpha_word1 = (discount * np.sum(self.bigram_counts > 0, axis=1)) / self.word_count_matrix\n",
        "        cont_probs = np.sum(self.bigram_counts > 0, axis=0) / np.sum(self.bigram_counts > 0)\n",
        "        return discounted_probs + alpha_word1[:, np.newaxis] * cont_probs\n",
        "\n",
        "    def calculate_probability_emotion_row(self, first_wrod, emotion_id=0):\n",
        "        non_zero_indices = np.nonzero(self.bigram_counts[self.vocabulary_index[first_wrod], :])[0]\n",
        "        emo_probs = np.zeros((self.vocab_size, 6))\n",
        "        for second_word_index in non_zero_indices:\n",
        "            k = self.emotion_scores(first_wrod + \" \" +  self.index_vocabulary[second_word_index])\n",
        "            prob_score = []\n",
        "            for label_score in range(6):\n",
        "                prob_score.append(k[label_score]['score'])\n",
        "            emo_probs[second_word_index] = np.array(prob_score)\n",
        "        first_word_mat = self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
        "        return first_word_mat[self.vocabulary_index[first_wrod],:] + emo_probs[:, emotion_id]\n",
        "\n",
        "    def emotion_scores(self, sample):\n",
        "        emotion=classifier(sample)\n",
        "        return emotion[0]\n",
        "\n",
        "    def learn(self, file_path):\n",
        "        self.build_corpus(file_path)\n",
        "        self.build_vocab()\n",
        "\n",
        "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
        "        self.word_count_matrix = np.array(list(self.word_count.values()))\n",
        "\n",
        "        for line in self.dataset:\n",
        "            for index in range(len(line) - 1):\n",
        "                first_word_index = self.vocabulary_index[line[index]]\n",
        "                second_word_index = self.vocabulary_index[line[index + 1]]\n",
        "                self.bigram_counts[first_word_index, second_word_index] += 1\n",
        "\n",
        "    def generate_samples(self, emotion_id = 0, num_samples = 50):\n",
        "            generated_samples = []\n",
        "            for _ in range(num_samples):\n",
        "                sample = self.generate_sample(emotion_id)\n",
        "                generated_samples.append(sample)\n",
        "            return generated_samples\n",
        "\n",
        "    def generate_sample(self, emotion_id = 0, max_length = 10):\n",
        "\n",
        "        start_word = np.random.choice(['i', 'im', 'ive'], p = [0.7, 0.2, 0.1])\n",
        "        current_word = start_word\n",
        "        sample = [current_word]\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "\n",
        "            current_word_index = self.vocabulary_index[current_word]\n",
        "            probabilities = self.calculate_probability_emotion_row(current_word, emotion_id)\n",
        "\n",
        "            if np.all(probabilities == 0):\n",
        "                break\n",
        "\n",
        "            probabilities /= probabilities.sum()\n",
        "\n",
        "            next_word_index = np.random.choice(self.vocab_size, p = probabilities)\n",
        "            next_word = self.index_vocabulary[next_word_index]\n",
        "\n",
        "            sample.append(next_word)\n",
        "            current_word = next_word\n",
        "\n",
        "        return ' '.join(sample)\n",
        "\n",
        "\n",
        "    def find_top_bigrams(self, num_top_bigrams=5):\n",
        "        top_bigrams = []\n",
        "        bigram_prob_scores = []\n",
        "\n",
        "        for i in range(self.vocab_size):\n",
        "            for j in range(self.vocab_size):\n",
        "                bigram = f\"{self.index_vocabulary[i]} {self.index_vocabulary[j]}\"\n",
        "                bigram_prob = self.bigram_probabilities[i, j]\n",
        "                bigram_prob_scores.append((bigram, bigram_prob))\n",
        "\n",
        "        sorted_bigrams = sorted(bigram_prob_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        top_bigrams = sorted_bigrams[:num_top_bigrams]\n",
        "\n",
        "        return top_bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ps5peL5DEBD"
      },
      "outputs": [],
      "source": [
        "corpus_path = 'corpus.txt'\n",
        "bigram_model = BigramLM_efficient()\n",
        "bigram_model.learn(corpus_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00036832412523020257\n",
            "0.501616467506957\n"
          ]
        }
      ],
      "source": [
        "bigram_model.build_probability_matrix(1)\n",
        "print(bigram_model.bigram_probabilities[bigram_model.vocabulary_index['hang'], bigram_model.vocabulary_index['out']])\n",
        "bigram_model.build_probability_matrix(2, 0.5)\n",
        "print(bigram_model.bigram_probabilities[bigram_model.vocabulary_index['hang'], bigram_model.vocabulary_index['out']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfGfDDkTfs0",
        "outputId": "f0f389ca-2029-4923-a195-2ba31d0c42ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 Bigrams (Before smoothing):\n",
            "\n",
            "Bigram: 'href http',   Probability: 1.0000\n",
            "Bigram: 'tychelle to',   Probability: 1.0000\n",
            "Bigram: 'hang out',   Probability: 1.0000\n",
            "Bigram: 'nonexistent social',   Probability: 1.0000\n",
            "Bigram: 'alex and',   Probability: 1.0000\n"
          ]
        }
      ],
      "source": [
        "bigram_model.build_probability_matrix(0)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (Before smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvjCbPGYUByB",
        "outputId": "3254800b-0fbc-4bdb-9dbe-20b5e45e6fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 Bigrams (After Laplace smoothing):\n",
            "\n",
            "Bigram: 'i feel',   Probability: 0.1104\n",
            "Bigram: 'feel like',   Probability: 0.0351\n",
            "Bigram: 'i am',   Probability: 0.0319\n",
            "Bigram: 'that i',   Probability: 0.0265\n",
            "Bigram: 'and i',   Probability: 0.0231\n"
          ]
        }
      ],
      "source": [
        "bigram_model.build_probability_matrix(1)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (After Laplace smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3RPAfebUB4f",
        "outputId": "0c11df05-dc5e-431d-caf2-43aba75e01af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 Bigrams (After Kneser Ney smoothing):\n",
            "\n",
            "Bigram: 'href http',   Probability: 0.9800\n",
            "Bigram: 'don t',   Probability: 0.9746\n",
            "Bigram: 'didn t',   Probability: 0.9722\n",
            "Bigram: 'sort of',   Probability: 0.9710\n",
            "Bigram: 'supposed to',   Probability: 0.9456\n"
          ]
        }
      ],
      "source": [
        "bigram_model.build_probability_matrix(2, 0.5)\n",
        "top_bigrams = bigram_model.find_top_bigrams()\n",
        "print(\"Top 5 Bigrams (After Kneser Ney smoothing):\")\n",
        "print()\n",
        "for bigram, prob in top_bigrams:\n",
        "    print(f\"Bigram: '{bigram}',   Probability: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z3iN8s4mLhg"
      },
      "outputs": [],
      "source": [
        "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "for emotion in range(len(emotions)):\n",
        "    generated_samples = bigram_model.generate_samples(num_samples = 50, emotion_id=emotion)\n",
        "    output_file = f'gen_{emotions[emotion]}.txt'\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for sample in generated_samples:\n",
        "            file.write(sample + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLrW1N1n7mk9"
      },
      "outputs": [],
      "source": [
        "corpus_path = 'corpus.txt'\n",
        "labels_path = 'labels.txt'\n",
        "\n",
        "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
        "    texts = [line.strip() for line in file]\n",
        "\n",
        "with open(labels_path, 'r', encoding='utf-8') as file:\n",
        "    labels = [line.strip() for line in file]\n",
        "\n",
        "\n",
        "X_train, y_train = texts, labels\n",
        "\n",
        "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "testing_samples = []\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_file_path = f'Generated files/gen_{emotion}_filtered.txt'\n",
        "    with open(emotion_file_path, 'r', encoding='utf-8') as file:\n",
        "        emotion_samples = [line.strip() for line in file]\n",
        "\n",
        "        X_test.extend(emotion_samples)\n",
        "        y_test.extend([emotion] * len(emotion_samples))\n",
        "\n",
        "y_test = list(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOW_BFWuerHT",
        "outputId": "f388ed6a-4504-4613-9c03-90aca4d29126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 74.0\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "svc_model = SVC(kernel = 'linear', C = 120, gamma = 0.002, break_ties = True, probability = True)\n",
        "svc_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = svc_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY3rrDnCJvgb",
        "outputId": "6fe845aa-0d0e-4d64-f5eb-1fdcb207507a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 115, 'gamma': 0.0096, 'kernel': 'rbf'}\n",
            "Accuracy with Grid Search: 75.33333333333333\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.42      0.56        50\n",
            "        fear       0.89      0.68      0.77        50\n",
            "         joy       0.59      0.68      0.63        50\n",
            "        love       0.85      0.90      0.87        50\n",
            "     sadness       0.59      0.86      0.70        50\n",
            "    surprise       0.92      0.98      0.95        50\n",
            "\n",
            "    accuracy                           0.75       300\n",
            "   macro avg       0.78      0.75      0.75       300\n",
            "weighted avg       0.78      0.75      0.75       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'C': [100, 115, 125],\n",
        "    'gamma': [0.001, 0.007, 0.0096],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "svc_model = SVC(break_ties = True, probability = True)\n",
        "\n",
        "grid_search = GridSearchCV(estimator = svc_model, param_grid = param_grid, cv = 5, scoring = 'accuracy')\n",
        "\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred_grid = best_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy_grid = accuracy_score(y_test, y_pred_grid)\n",
        "classification_rep = classification_report(y_test, y_pred_grid)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy with Grid Search:\", accuracy_grid * 100)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
