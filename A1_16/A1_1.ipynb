{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'I$': 1, 'am$': 1, 'sitting$': 1, 'on$': 1, 'this$': 2, 'chair$': 2, 'The$': 2, 'made$': 1, 'of$': 4, 'wood$': 1, 'Wood$': 1, 'definitely$': 1, 'flamable$': 1, 'room$': 1, 'Sridhar$': 1, '302$': 1, 'lives$': 1, 'with$': 1, '3$': 1, 'other$': 1, 'roommates$': 1, 'Some$': 1, 'has$': 2, 'been$': 2, 'written$': 2, 'in$': 2, 'the$': 2, 'library$': 1, 'and$': 1, 'some$': 1, 'it$': 1, 'guest$': 1, 'house$': 1}\n",
      "Merge Rules: [('e', '$'), ('s', '$'), ('n', '$'), ('i', 't'), ('t', 'h'), ('i', 's$'), ('h', 'a'), ('r', '$'), ('o', 'f'), ('of', '$'), ('o', 'o'), ('e', 'n$'), ('it', 't'), ('h', 'e$'), ('m', 'a'), ('d', '$'), ('m', '$'), ('i', 'n'), ('th', 'is$'), ('c', 'ha'), ('cha', 'i'), ('chai', 'r$'), ('T', 'he$'), ('oo', 'd$'), ('y', '$'), ('r', 'oo'), ('l', 'i'), ('e', 's$'), ('o', 'm'), ('om', 'e$'), ('ha', 's$'), ('b', 'e'), ('be', 'en$'), ('w', 'r'), ('wr', 'itt'), ('writt', 'en$'), ('I', '$'), ('a', 'm$'), ('s', 'itt'), ('sitt', 'in'), ('sittin', 'g'), ('sitting', '$'), ('o', 'n$'), ('ma', 'd'), ('mad', 'e$'), ('w', 'ood$'), ('W', 'ood$'), ('d', 'e'), ('de', 'f'), ('def', 'in'), ('defin', 'it'), ('definit', 'e'), ('definite', 'l'), ('definitel', 'y$'), ('f', 'l'), ('fl', 'a'), ('fla', 'ma'), ('flama', 'b'), ('flamab', 'l'), ('flamabl', 'e$'), ('S', 'r'), ('Sr', 'i'), ('Sri', 'd'), ('Srid', 'ha'), ('Sridha', 'r$'), ('3', '0'), ('30', '2'), ('302', '$'), ('li', 'v'), ('liv', 'es$'), ('w', 'ith'), ('with', '$'), ('3', '$'), ('o', 'th'), ('oth', 'e'), ('othe', 'r$'), ('room', 'ma'), ('roomma', 't'), ('roommat', 'es$'), ('S', 'ome$'), ('li', 'b'), ('lib', 'r'), ('libr', 'a'), ('libra', 'r'), ('librar', 'y$'), ('a', 'n'), ('an', 'd$'), ('s', 'ome$'), ('it', '$'), ('g', 'u'), ('gu', 'e'), ('gue', 's'), ('gues', 't'), ('guest', '$'), ('h', 'o'), ('ho', 'u'), ('hou', 's'), ('hous', 'e$')]\n",
      "Tokens: ['n', 'g', 'r', 'a', 'y', 'v', 'c', 'T', 'I', 'W', 'i', '3', 's', 'h', 'l', 'o', '2', 'd', 'f', 'e', 'm', '0', 't', 'S', 'b', 'w', 'u', '$', 'e$', 's$', 'n$', 'it', 'th', 'is$', 'ha', 'r$', 'of', 'of$', 'oo', 'en$', 'itt', 'he$', 'ma', 'd$', 'm$', 'in', 'this$', 'cha', 'chai', 'chair$', 'The$', 'ood$', 'y$', 'roo', 'li', 'es$', 'om', 'ome$', 'has$', 'be', 'been$', 'wr', 'writt', 'written$', 'I$', 'am$', 'sitt', 'sittin', 'sitting', 'sitting$', 'on$', 'mad', 'made$', 'wood$', 'Wood$', 'de', 'def', 'defin', 'definit', 'definite', 'definitel', 'definitely$', 'fl', 'fla', 'flama', 'flamab', 'flamabl', 'flamable$', 'Sr', 'Sri', 'Srid', 'Sridha', 'Sridhar$', '30', '302', '302$', 'liv', 'lives$', 'with', 'with$', '3$', 'oth', 'othe', 'other$', 'roomma', 'roommat', 'roommates$', 'Some$', 'lib', 'libr', 'libra', 'librar', 'library$', 'an', 'and$', 'some$', 'it$', 'gu', 'gue', 'gues', 'guest', 'guest$', 'ho', 'hou', 'hous', 'house$']\n",
      "Tokenised Version: k,an,y,e$,made$,t,a,i,l,o,r$,s,w,i,f,t,$,f,a,m,o,u,s$\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merge_rules = []\n",
    "        self.token_list=[]\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        pattern = re.compile(re.escape(' '.join(pair)))\n",
    "        max_freq_word = max(vocab, key=vocab.get)\n",
    "        vocab2 = {pattern.sub(''.join(pair), word): freq for word, freq in vocab.items()}\n",
    "        vocab2.pop(' '.join(pair), None)\n",
    "        vocab2.pop(pair[0], None)\n",
    "        vocab2.pop(pair[1], None)\n",
    "        return vocab2\n",
    "    \n",
    "    def get_stats(self, vocab):\n",
    "        pairs_count = defaultdict(int)\n",
    "        for word, frequency in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs_count[pair] += frequency\n",
    "        return dict(pairs_count)\n",
    "    \n",
    "    def build_vocab(self, corpus, num_merges):\n",
    "        words = corpus.split()\n",
    "        tokens = [' '.join(list(word) + ['$']) for word in words]\n",
    "        all_characters = ''.join([char for word in tokens for char in word if char != ' '])\n",
    "        self.token_list = list(set(all_characters))\n",
    "        token_counts = Counter(tokens)\n",
    "        self.vocab = dict(token_counts)\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(self.vocab)  # Step 2\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.token_list.append(best[0]+best[1])\n",
    "            self.merge_rules.append(best)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "        return self.vocab, self.merge_rules,self.token_list\n",
    "    \n",
    "    def tokenize(self, input_text):       \n",
    "        words = input_text.split()\n",
    "        tokens = [' '.join(list(word) + ['$']) for word in words]\n",
    "        for rule in self.merge_rules:\n",
    "            tokens = [token.replace(' '.join(rule), ''.join(rule)) for token in tokens]\n",
    "        tokens2=[]\n",
    "        for i in tokens:\n",
    "            tokens2.append(i.replace(\" \",\",\"))\n",
    "        str2=\"\"\n",
    "        for i in range(0,len(tokens2)):\n",
    "            if(i!=len(tokens2)-1):            \n",
    "                str2+=tokens2[i]+\",\"\n",
    "            else:\n",
    "                str2+=tokens2[i]\n",
    "        return str2\n",
    "    \n",
    "#################################################################\n",
    "    \n",
    "file = open('corpus2.txt', 'r')\n",
    "corpus = file.read().replace('\\n', ' ')\n",
    "# print(corpus)\n",
    "num_merges = 100\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "vocabulary, merge_rules,tok = tokenizer.build_vocab(corpus, num_merges)\n",
    "\n",
    "tokenized=tokenizer.tokenize(\"kanye made tailor swift famous\")\n",
    "\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Merge Rules:\", merge_rules)\n",
    "print(\"Tokens:\",tok)\n",
    "print(\"Tokenised Version:\",tokenized)\n",
    "\n",
    "f = open(  \"merge_rules.txt\", \"w\")\n",
    "for i in merge_rules:\n",
    "    f.write(i[0] + \",\" + i[1] + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open( \"tokens.txt\", \"w\")\n",
    "for i in tok:\n",
    "    f.write(i + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"tokenized.txt\", \"w\")\n",
    "n = int(input(\"Enter the number of sentences you want to tokenize: \"))\n",
    "for i in range(n):\n",
    "    s = input(\"Enter the sentence: \")\n",
    "    f.write(tokenizer.tokenize(s) + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
