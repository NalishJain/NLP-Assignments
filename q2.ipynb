{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.4.0-1-cp310-cp310-macosx_12_0_arm64.whl (10.6 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install numpy\n",
    "# !pip install tensorflow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at bhadresh-savani/distilbert-base-uncased-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "/Users/nalishjain/Documents/GitHub/NLP-Assignments/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np \n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLM:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 0\n",
    "        self.vocabulary_index = {}\n",
    "        self.word_count = {}\n",
    "        self.index_vocabulary = {}\n",
    "        self.bigram_counts = None\n",
    "        self.bigram_probabilities = None\n",
    "        self.dataset = None\n",
    "\n",
    "\n",
    "    def build_corpus(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sentences = [line.strip().split() for line in file]\n",
    "        self.dataset =  sentences\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        for line in self.dataset:\n",
    "            for word in line:\n",
    "                if word not in self.vocabulary_index:\n",
    "                    self.vocabulary_index[word] = self.vocab_size\n",
    "                    self.index_vocabulary[self.vocab_size] = word\n",
    "                    self.word_count[word] = 0\n",
    "                    self.vocab_size += 1\n",
    "                self.word_count[word] += 1\n",
    "    def learn(self, file_path):\n",
    "        # Build vocabulary and initialize bigram counts\n",
    "        self.build_corpus(file_path)\n",
    "        self.build_vocab()\n",
    "\n",
    "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
    "\n",
    "        for line in self.dataset:\n",
    "            for index in range(len(line) - 1):\n",
    "                first_word_index = self.vocabulary_index[line[index]]\n",
    "                second_word_index = self.vocabulary_index[line[index + 1]]\n",
    "                self.bigram_counts[first_word_index, second_word_index] += 1\n",
    "\n",
    "    def calculate_probability(self, word1, word2):\n",
    "        return self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]/self.word_count[word1]\n",
    "    \n",
    "    def laplace_smoothing(self, word1, word2):\n",
    "        return (self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]] + 1)/(self.word_count[word1] + self.vocab_size)\n",
    "\n",
    "    def kneser_ney_smoothing(self, word1, word2, discount = 0):\n",
    "        discounted_prob = max(self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]-discount, 0)/self.word_count[word1]\n",
    "        alpha_word1 = (discount* np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))/self.word_count[word1]\n",
    "        cont_prob = np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0)/np.sum(self.bigram_counts > 0)\n",
    "        print(\"alpha_word1\", alpha_word1)\n",
    "        print(\"discounted_prob\",discounted_prob )\n",
    "        print(\"self.word_count[word1]\", self.word_count[word1])\n",
    "        print(\"np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))\", np.sum(self.bigram_counts[self.vocabulary_index[word1], :] > 0))\n",
    "        print(\"np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0)\", np.sum(self.bigram_counts[:, self.vocabulary_index[word2]] > 0))\n",
    "        print(\"np.sum(self.bigram_counts > 0)\", np.sum(self.bigram_counts > 0))\n",
    "\n",
    "        return discounted_prob + alpha_word1*cont_prob\n",
    "    \n",
    "    def emotion_scores(self, sample): \n",
    "        emotion=classifier(sample)\n",
    "        return emotion[0]\n",
    "\n",
    "    def calculate_probability_emotion(self, word1, word2, emotion_id):\n",
    "        emo_prob = self.emotion_scores(word1 + \" \" +word2)\n",
    "        return self.bigram_counts[self.vocabulary_index[word1], self.vocabulary_index[word2]]/self.word_count[word1] + emo_prob[emotion_id]['score']\n",
    "    \n",
    "    def build_probability_matrix(self, mode, discount = 0, emotion_id = 0):\n",
    "        self.bigram_probabilities =  np.zeros((self.vocab_size, self.vocab_size), dtype=float)\n",
    "        if mode == 0:\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    self.bigram_probabilities[i, j] =  self.calculate_probability(self.index_vocabulary[i], self.index_vocabulary[j])\n",
    "\n",
    "        elif mode == 1:\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    self.bigram_probabilities[i, j] =  self.laplace_smoothing(self.index_vocabulary[i], self.index_vocabulary[j])\n",
    "                    \n",
    "        elif mode == 2:\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    print(i, j)\n",
    "                    self.bigram_probabilities[i, j] =  self.kneser_ney_smoothing(self.index_vocabulary[i], self.index_vocabulary[j], discount= discount) \n",
    "\n",
    "        else:\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    print(i, j)\n",
    "                    self.bigram_probabilities[i, j] =  self.calculate_probability_emotion(self.index_vocabulary[i], self.index_vocabulary[j], emotion_id= emotion_id)                              \n",
    "# Display the formed corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramLM = BigramLM()\n",
    "bigramLM.learn(\"corpus.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.0006333347409963608},\n",
       " {'label': 'joy', 'score': 0.00038153710193000734},\n",
       " {'label': 'love', 'score': 0.00023734646674711257},\n",
       " {'label': 'anger', 'score': 0.9974260926246643},\n",
       " {'label': 'fear', 'score': 0.0011390680447220802},\n",
       " {'label': 'surprise', 'score': 0.00018260569777339697}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigramLM.emotion_scores('violent')\n",
    "# 0 -> sadness\n",
    "# 1 -> joy\n",
    "# 2 ->'love\n",
    "# 3 -> 'anger\n",
    "# 4-> fear\n",
    "# 5-> surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM_efficient:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 0\n",
    "        self.vocabulary_index = {}\n",
    "        self.word_count = {}\n",
    "        self.index_vocabulary = {}\n",
    "        self.bigram_counts = None\n",
    "        self.bigram_probabilities = None\n",
    "        self.dataset = None\n",
    "\n",
    "\n",
    "    def build_corpus(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sentences = [line.strip().split() for line in file]\n",
    "        self.dataset =  sentences\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        for line in self.dataset:\n",
    "            for word in line:\n",
    "                if word not in self.vocabulary_index:\n",
    "                    self.vocabulary_index[word] = self.vocab_size\n",
    "                    self.index_vocabulary[self.vocab_size] = word\n",
    "                    self.word_count[word] = 0\n",
    "                    self.vocab_size += 1\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "    def build_probability_matrix(self, mode, discount=0, emotion_id=0):\n",
    "        self.bigram_probabilities = np.zeros((self.vocab_size, self.vocab_size), dtype=float)\n",
    "\n",
    "        if mode == 0:\n",
    "            self.bigram_probabilities = self.calculate_probability_matrix()\n",
    "        elif mode == 1:\n",
    "            self.bigram_probabilities = self.laplace_smoothing_matrix()\n",
    "        elif mode == 2:\n",
    "            self.bigram_probabilities = self.kneser_ney_smoothing_matrix(discount=discount)\n",
    "     \n",
    "\n",
    "    def calculate_probability_matrix(self):\n",
    "        return self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
    "\n",
    "    def laplace_smoothing_matrix(self):\n",
    "        return (self.bigram_counts + 1) / (self.word_count_matrix[:, np.newaxis] + self.vocab_size)\n",
    "\n",
    "    def kneser_ney_smoothing_matrix(self, discount=0):\n",
    "        discounted_probs = np.maximum(self.bigram_counts - discount, 0) / self.word_count_matrix[:, np.newaxis]\n",
    "        alpha_word1 = (discount * np.sum(self.bigram_counts > 0, axis=1)) / self.word_count_matrix\n",
    "        cont_probs = np.sum(self.bigram_counts > 0, axis=0) / np.sum(self.bigram_counts > 0)\n",
    "        return discounted_probs + alpha_word1[:, np.newaxis] * cont_probs\n",
    "\n",
    "    def calculate_probability_emotion_row(self, first_wrod, emotion_id=0):\n",
    "        non_zero_indices = np.nonzero(self.bigram_counts[self.vocabulary_index[first_wrod], :])[0]\n",
    "        emo_probs = np.zeros((self.vocab_size, 6))\n",
    "        for second_word_index in non_zero_indices:\n",
    "            k = self.emotion_scores(first_wrod + \" \" +  self.index_vocabulary[second_word_index])\n",
    "            prob_score = []\n",
    "            for label_score in range(6):\n",
    "                prob_score.append(k[label_score]['score'])\n",
    "            emo_probs[second_word_index] = np.array(prob_score)\n",
    "        first_word_mat = self.bigram_counts / self.word_count_matrix[:, np.newaxis]\n",
    "        return first_word_mat[self.vocabulary_index[first_wrod],:] + emo_probs[:, emotion_id]\n",
    "    \n",
    "    def emotion_scores(self, sample): \n",
    "        emotion=classifier(sample)\n",
    "        return emotion[0]\n",
    "\n",
    "    def learn(self, file_path):\n",
    "        self.build_corpus(file_path)\n",
    "        self.build_vocab()\n",
    "\n",
    "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
    "        self.word_count_matrix = np.array(list(self.word_count.values()))\n",
    "\n",
    "        for line in self.dataset:\n",
    "            for index in range(len(line) - 1):\n",
    "                first_word_index = self.vocabulary_index[line[index]]\n",
    "                second_word_index = self.vocabulary_index[line[index + 1]]\n",
    "                self.bigram_counts[first_word_index, second_word_index] += 1\n",
    "                \n",
    "    def generate_samples(self, emotion_id = 0, num_samples = 50):\n",
    "            generated_samples = []\n",
    "            for _ in range(num_samples):\n",
    "                sample = self.generate_sample(emotion_id)\n",
    "                generated_samples.append(sample)\n",
    "            return generated_samples\n",
    "\n",
    "    def generate_sample(self, emotion_id = 0, max_length = 7):\n",
    "\n",
    "        start_word = np.random.choice(['i', 'im', 'ive'], p=[0.7, 0.2, 0.1])\n",
    "        current_word = start_word\n",
    "        sample = [current_word]\n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            current_word_index = self.vocabulary_index[current_word]\n",
    "\n",
    "            probabilities = self.calculate_probability_emotion_row(current_word, emotion_id)\n",
    "\n",
    "            # Check if all probabilities are zero\n",
    "            if np.all(probabilities == 0):\n",
    "                break\n",
    "\n",
    "            probabilities /= probabilities.sum()\n",
    "\n",
    "            next_word_index = np.random.choice(self.vocab_size, p=probabilities)\n",
    "            next_word = self.index_vocabulary[next_word_index]\n",
    "\n",
    "            sample.append(next_word)\n",
    "            current_word = next_word\n",
    "\n",
    "        return ' '.join(sample)                \n",
    "# Example usage:\n",
    "corpus_path = 'corpus.txt'\n",
    "bigram_model = BigramLM_efficient()\n",
    "bigram_model.learn(corpus_path)\n",
    "# bigram_model.build_probability_matrix(mode=1, discount=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "length = 10\n",
    "for emotion in emotions:\n",
    "    generated_samples = bigram_model.generate_samples(num_samples = length, emotion_id=emotion)\n",
    "    output_file = f'gen_5_{emotions[emotion]}.txt'\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for sample in generated_samples:\n",
    "            file.write(sample + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load training data\n",
    "corpus_path = 'corpus.txt'\n",
    "labels_path = 'labels.txt'\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "    texts = [line.strip() for line in file]\n",
    "\n",
    "with open(labels_path, 'r', encoding='utf-8') as file:\n",
    "    labels = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "X_train, y_train = texts, labels\n",
    "\n",
    "# Load testing data (generated samples for each emotion)\n",
    "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "testing_samples = []\n",
    "\n",
    "X_test = []  # List to store text samples\n",
    "y_test = []  # List to store corresponding emotions\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_file_path = f'Generated files/gen_{emotion}_filtered.txt'\n",
    "    with open(emotion_file_path, 'r', encoding='utf-8') as file:\n",
    "        emotion_samples = [line.strip() for line in file]\n",
    "\n",
    "        # Extend X_test with emotion_samples\n",
    "        X_test.extend(emotion_samples)\n",
    "\n",
    "        # Extend y_test with the corresponding emotion labels\n",
    "        y_test.extend([emotion] * len(emotion_samples))\n",
    "\n",
    "# Convert y_test to a list\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean = False), SVC())\n",
    "\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (2,2)],\n",
    "    'tfidfvectorizer__max_features': [500, 1000, 3000, None],\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv = 5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy on Testing Data:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.83      0.40      0.54        50\n",
      "        fear       0.89      0.68      0.77        50\n",
      "         joy       0.58      0.68      0.62        50\n",
      "        love       0.85      0.90      0.87        50\n",
      "     sadness       0.59      0.86      0.70        50\n",
      "    surprise       0.92      0.98      0.95        50\n",
      "\n",
      "    accuracy                           0.75       300\n",
      "   macro avg       0.78      0.75      0.74       300\n",
      "weighted avg       0.78      0.75      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "svc_model = SVC(kernel = 'rbf', C = 113, gamma = 0.0095, break_ties = True, probability = True)\n",
    "svc_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = svc_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n",
    "# Print the results\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
