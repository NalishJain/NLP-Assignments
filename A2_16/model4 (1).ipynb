{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk8C7ArLYQPY",
        "outputId": "c1c7dec0-376e-4ca2-acb2-49015c95fe36"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import pickle\n",
        "# from google.colab import drive\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.utils import tf_utils\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import keras.layers as L\n",
        "# import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u2weMAvnYQMz"
      },
      "outputs": [],
      "source": [
        "with open('Json Task1/NER_train.json', 'r') as f:\n",
        "    task1_train_data = json.load(f)\n",
        "\n",
        "with open('Json Task1/NER_val.json', 'r') as f:\n",
        "    task1_val_data = json.load(f)\n",
        "\n",
        "with open('Json Task1/NER_test.json', 'r') as f:\n",
        "    task1_test_data = json.load(f)\n",
        "\n",
        "with open('Json Task2/ATE_train.json', 'r') as f:\n",
        "    task2_train_data = json.load(f)\n",
        "\n",
        "with open('Json Task2/ATE_val.json', 'r') as f:\n",
        "    task2_val_data = json.load(f)\n",
        "\n",
        "with open('Json Task2/ATE_test.json', 'r') as f:\n",
        "    task2_test_data = json.load(f)\n",
        "\n",
        "with open('glove_embedding.pkl', 'rb') as pickle_file:\n",
        "    glove_embeddings = pickle.load(pickle_file)\n",
        "    \n",
        "with open('fast_text_embedding.pkl', 'rb') as pickle_file:\n",
        "    fast_text_embedding = pickle.load(pickle_file)\n",
        "\n",
        "with open('word2vec_embeddings.pkl', 'rb') as pickle_file:\n",
        "    word2vec_embeddings = pickle.load(pickle_file)\n",
        "\n",
        "bio_mapping_task1 = {'B_ORG': 0, 'I_ORG': 1, 'B_RESPONDENT': 2, 'I_RESPONDENT': 3, 'B_JUDGE': 4, 'I_JUDGE': 5,\n",
        "               'B_STATUTE': 6, 'I_STATUTE': 7, 'B_OTHER_PERSON': 8, 'I_OTHER_PERSON': 9, 'B_COURT': 10, 'I_COURT': 11,\n",
        "               'B_GPE': 12, 'I_GPE': 13, 'B_PETITIONER': 14, 'I_PETITIONER': 15, 'B_WITNESS': 16, 'I_WITNESS': 17,\n",
        "               'B_CASE_NUMBER': 18, 'I_CASE_NUMBER': 19, 'B_PRECEDENT': 20, 'I_PRECEDENT': 21, 'B_DATE': 22, 'I_DATE': 23,\n",
        "               'B_PROVISION': 24, 'I_PROVISION': 25, 'O': 26}\n",
        "bio_mapping_task2 = {'O' : 0, 'I' : 1, 'B' : 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FJ7P6O1YQKt",
        "outputId": "f38a7f37-9a95-4f77-b5e9-0cd057f2a850"
      },
      "outputs": [],
      "source": [
        "# word2vec_model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IdEvm8ptYQIF",
        "outputId": "ef064699-29bf-4376-a7c2-fa731b42dbc1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'word2vec_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mword2vec_model\u001b[49m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[1;32m      2\u001b[0m word2vec_u_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((word2vec, np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m300\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m word_to_index_w2v \u001b[38;5;241m=\u001b[39m {word: index \u001b[38;5;28;01mfor\u001b[39;00m index, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(word2vec_model\u001b[38;5;241m.\u001b[39mindex_to_key)}\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word2vec_model' is not defined"
          ]
        }
      ],
      "source": [
        "# word2vec = torch.FloatTensor(word2vec_model.vectors)\n",
        "# word2vec_u_ = np.concatenate((word2vec, np.zeros((1, 300), dtype = 'float32')), axis = 0)\n",
        "\n",
        "# word_to_index_w2v = {word: index for index, word in enumerate(word2vec_model.index_to_key)}\n",
        "# word_to_index_w2v['<unk>'] = len(word_to_index_w2v)\n",
        "# vocab_size_w2v = len(word_to_index_w2v)\n",
        "\n",
        "# word2vec_embeddings = {word: word2vec_u_[index] for word, index in word_to_index_w2v.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_to_index_w2v = {}\n",
        "ind = 0\n",
        "word2vec_embeddings['<unk>'] = np.zeros((1, 300), dtype = 'float32')\n",
        "\n",
        "for word in word2vec_embeddings.keys():\n",
        "  word_to_index_w2v[word] = ind\n",
        "  ind = ind + 1\n",
        "\n",
        "vocab_size_w2v = len(word_to_index_w2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qbWndtH1YQFO"
      },
      "outputs": [],
      "source": [
        "word_to_index_glove = {}\n",
        "ind = 0\n",
        "glove_embeddings['<unk>'] = np.zeros((1, 300), dtype = 'float32')\n",
        "\n",
        "for word in glove_embeddings.keys():\n",
        "  word_to_index_glove[word] = ind\n",
        "  ind = ind + 1\n",
        "\n",
        "vocab_size_glove = len(word_to_index_glove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yuVmc9fIYQC3"
      },
      "outputs": [],
      "source": [
        "word_to_index_fasttext = {}\n",
        "ind = 0\n",
        "fast_text_embedding['<unk>'] = np.zeros((1, 300), dtype = 'float32')\n",
        "\n",
        "for word in fast_text_embedding.keys():\n",
        "  word_to_index_fasttext[word] = ind\n",
        "  ind = ind + 1\n",
        "\n",
        "vocab_size_fasttext = len(word_to_index_fasttext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Iu353ya2YP-f"
      },
      "outputs": [],
      "source": [
        "def process_data(dataset,task,split, embedding_type):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    max_length_train_1 = 80\n",
        "    max_length_train_2 = 83\n",
        "    # max_length_test_1 = 52\n",
        "    # max_length_test_2 = 71\n",
        "    max_seq_len = 0\n",
        "\n",
        "    if task == 1:\n",
        "        bio_mapping = bio_mapping_task1\n",
        "        max_seq_len = max_length_train_1\n",
        "    else:\n",
        "        bio_mapping = bio_mapping_task2\n",
        "        max_seq_len = max_length_train_2\n",
        "\n",
        "# choosing embedding type\n",
        "    if embedding_type == \"word2vec\":\n",
        "        word_to_index = word_to_index_w2v\n",
        "\n",
        "    if embedding_type == \"glove\":\n",
        "        word_to_index = word_to_index_glove\n",
        "\n",
        "    else:\n",
        "        word_to_index = word_to_index_fasttext\n",
        "\n",
        "    for index in dataset.keys():\n",
        "\n",
        "        dic = dataset[index]\n",
        "        tags = [bio_mapping[label] for label in dic['labels']]\n",
        "        sentence = dic['text'].split(' ')\n",
        "        sequence = []\n",
        "        for word in sentence:\n",
        "            if word in word_to_index:\n",
        "                sequence.append(word_to_index[word])\n",
        "            else:\n",
        "                sequence.append(word_to_index['<unk>'])\n",
        "        if(len(sequence) >= max_seq_len):\n",
        "            continue\n",
        "\n",
        "        if(len(sequence) < max_seq_len):\n",
        "            for i in range(len(sequence), max_seq_len):\n",
        "                sequence.append(word_to_index['<unk>'])\n",
        "                tags.append(bio_mapping['O'])\n",
        "                # tags.append(3)\n",
        "\n",
        "        sentences.append(sequence)\n",
        "        labels.append(tags)\n",
        "\n",
        "    tag_size = len(bio_mapping)\n",
        "    labels = [to_categorical(i, num_classes = tag_size) for i in labels]\n",
        "\n",
        "    return np.array(sentences), np.asarray(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HpfSchmyYP2c"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = process_data(task1_train_data, 1, split = \"train\", embedding_type = \"glove\")\n",
        "X_val, y_val = process_data(task1_val_data, 1, split = \"val\", embedding_type = \"glove\")\n",
        "X_test, y_test = process_data(task1_test_data, 1, split = \"test\", embedding_type = \"glove\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cJqv900BYj1u"
      },
      "outputs": [],
      "source": [
        "class CRF(L.Layer):\n",
        "    def __init__(self,\n",
        "                 output_dim,\n",
        "                 sparse_target=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.output_dim = int(output_dim)\n",
        "        self.sparse_target = sparse_target\n",
        "        self.input_spec = L.InputSpec(min_ndim=3)\n",
        "        self.supports_masking = False\n",
        "        self.sequence_lengths = None\n",
        "        self.transitions = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        f_shape = tf.TensorShape(input_shape)\n",
        "        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
        "\n",
        "        self.input_spec = input_spec\n",
        "        self.transitions = self.add_weight(name='transitions',\n",
        "                                           shape=[self.output_dim, self.output_dim],\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
        "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        if sequence_lengths is not None:\n",
        "            assert len(sequence_lengths.shape) == 2\n",
        "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
        "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
        "            assert seq_len_shape[1] == 1\n",
        "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
        "        else:\n",
        "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
        "                tf.shape(inputs)[1]\n",
        "            )\n",
        "\n",
        "        viterbi_sequence, _ = crf_decode(sequences,\n",
        "                                         self.transitions,\n",
        "                                         self.sequence_lengths)\n",
        "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
        "        return K.in_train_phase(sequences, output)\n",
        "\n",
        "    @property\n",
        "    def loss(self):\n",
        "        def crf_loss(y_true, y_pred):\n",
        "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
        "            log_likelihood, self.transitions = crf_log_likelihood(\n",
        "                y_pred,\n",
        "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
        "                self.sequence_lengths,\n",
        "                transition_params=self.transitions,\n",
        "            )\n",
        "            return tf.reduce_mean(-log_likelihood)\n",
        "        return crf_loss\n",
        "\n",
        "    @property\n",
        "    def accuracy(self):\n",
        "        def viterbi_accuracy(y_true, y_pred):\n",
        "            # -1e10 to avoid zero at sum(mask)\n",
        "            mask = K.cast(\n",
        "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
        "            shape = tf.shape(y_pred)\n",
        "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
        "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
        "            if self.sparse_target:\n",
        "                y_true = K.argmax(y_true, 2)\n",
        "            y_pred = K.cast(y_pred, 'int32')\n",
        "            y_true = K.cast(y_true, 'int32')\n",
        "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
        "            return K.sum(corrects * mask) / K.sum(mask)\n",
        "        return viterbi_accuracy\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
        "        return input_shape[:2] + (self.output_dim,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'sparse_target': self.sparse_target,\n",
        "            'supports_masking': self.supports_masking,\n",
        "            'transitions': K.eval(self.transitions)\n",
        "        }\n",
        "        base_config = super(CRF, self).get_config()\n",
        "        return dict(base_config, **config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AV8XTzheYkNa"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, max_length, embedding_dim, word_index, tag_index, emb_type):\n",
        "\n",
        "    if emb_type == \"word2vec\":\n",
        "        embeddings_index = word2vec_embeddings\n",
        "\n",
        "    elif emb_type == \"glove\":\n",
        "        embeddings_index = glove_embeddings\n",
        "    else:\n",
        "        embeddings_index = fast_text_embedding\n",
        "\n",
        "    embeddings_index = {}\n",
        "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i > vocab_size:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim,\n",
        "                    input_length = max_length, weights = [embeddings_matrix], mask_zero = True))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(units = 100, return_sequences = True,\n",
        "                             recurrent_dropout = 0.01)))\n",
        "    model.add(Bidirectional(LSTM(units = 30, return_sequences = True,\n",
        "                             recurrent_dropout = 0.1)))\n",
        "    model.add(TimeDistributed(Dense(len(tag_index))))\n",
        "\n",
        "    crf = CRF(len(tag_index), sparse_target=True)\n",
        "    model.add(crf)\n",
        "    # model.compile(optimizer = 'adam', loss = crf.loss, metrics = [crf.accuracy])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d8WWSRGtjMgv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_f1_score(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_argmax = np.argmax(y_pred, axis=-1)\n",
        "    y_true_argmax = np.argmax(y_test, axis=-1)\n",
        "    # print(X_test.shape, y_pred.shape, y_test.shape, y_pred_argmax.shape, y_true_argmax.shape)\n",
        "    f1 = f1_score(y_true_argmax.flatten(), y_pred_argmax.flatten(), average='macro')\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AiD_EXnbBxVj"
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_val, y_val, X_test, y_test):\n",
        "        super().__init__()\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_f1_scores = []\n",
        "        self.val_f1_scores = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Training loss\n",
        "        train_loss = logs['loss']\n",
        "        self.train_losses.append(train_loss)\n",
        "\n",
        "        # Training F1 score\n",
        "        train_f1_score = calculate_f1_score(self.model, X_train, y_train)\n",
        "        self.train_f1_scores.append(train_f1_score)\n",
        "\n",
        "        # Validation loss\n",
        "        t = self.model.evaluate(self.X_val, self.y_val, verbose=0)\n",
        "        # print(t)\n",
        "        val_loss = t[0]\n",
        "        self.val_losses.append(val_loss)\n",
        "\n",
        "        # Validation F1 score\n",
        "        val_f1_score = calculate_f1_score(self.model, self.X_val, self.y_val)\n",
        "        self.val_f1_scores.append(val_f1_score)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'Epoch {epoch + 1} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Train F1 Score: {train_f1_score:.4f} - Val F1 Score: {val_f1_score:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJzNgxRXYJQA",
        "outputId": "cdf4ba81-2288-4a9a-fd94-b10f14ba2adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow is using CPU.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"TensorFlow is using GPU.\")\n",
        "else:\n",
        "    print(\"TensorFlow is using CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQR-Pr2ZB2q9",
        "outputId": "f3f739d5-cc06-4a2a-c3ca-d0a51c70da6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "244/244 [==============================] - 15s 60ms/step\n",
            "44/44 [==============================] - 3s 59ms/step\n",
            "Epoch 1 - Train Loss: 23.6749 - Val Loss: 166.2344 - Train F1 Score: 0.0982 - Val F1 Score: 0.0966\n"
          ]
        }
      ],
      "source": [
        "model = create_model(vocab_size = vocab_size_fasttext, max_length = 80, embedding_dim = 300, word_index = word_to_index_glove, tag_index = bio_mapping_task1, emb_type = \"glove\")\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=1e-3), loss=model.layers[-1].loss, metrics=[model.layers[-1].accuracy])\n",
        "\n",
        "metrics_callback = MetricsCallback(X_val, y_val, X_test, y_test)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size = 16, epochs = 11, validation_data = (X_val, y_val), verbose = 0, callbacks = [metrics_callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/29 [==============================] - 2s 61ms/step\n",
            "Test Loss: 169.0938 - Test Accuracy: 0.0985 - Test F1 Score: 0.5242\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose = 0)\n",
        "test_f1_score = calculate_f1_score(model, X_test, y_test)\n",
        "print(f'Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f} - Test F1 Score: {test_f1_score:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
