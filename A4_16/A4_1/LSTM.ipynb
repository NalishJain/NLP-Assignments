{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install tqdm torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dict.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "with open('val_dict.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "emotion_ids = {'neutral' : 0, 'joy' : 1, 'anger' : 2, 'surprise' : 3, 'sadness' : 4, 'fear' : 5, 'disgust' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data():\n",
    "    max_length = 25\n",
    "    pad_word = 'PAD'\n",
    "    pad_emotion = 'neutral'\n",
    "    data = [train_data, val_data]\n",
    "    dict_ = {}\n",
    "\n",
    "    for task_data in data:\n",
    "        remove_keys = []\n",
    "        for key in task_data:\n",
    "            if len(task_data[key][0]) not in dict_:\n",
    "                dict_[len(task_data[key][0])] = 0\n",
    "            dict_[len(task_data[key][0])] += 1\n",
    "            # checking_nan\n",
    "            for step in range(len(task_data[key][3])):\n",
    "                if task_data[key][3][step] is None:\n",
    "                    remove_keys.append(key)\n",
    "                    \n",
    "              \n",
    "            for step in range(len(task_data[key][0]), max_length):\n",
    "                task_data[key][0].append(pad_word)\n",
    "                task_data[key][1].append(np.zeros(768))\n",
    "                task_data[key][2].append(pad_emotion)\n",
    "                task_data[key][3].append(0)\n",
    "        \n",
    "        for key in remove_keys:\n",
    "            if key in task_data:\n",
    "                del task_data[key] \n",
    "\n",
    "\n",
    "pad_data()\n",
    "train_data = {new_key: train_data[old_key] for new_key, (old_key, _) in enumerate(train_data.items())}\n",
    "val_data = {new_key: val_data[old_key] for new_key, (old_key, _) in enumerate(val_data.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim, num_classes):\n",
    "#         super(GRUModel, self).__init__()\n",
    "#         self.gru1 = nn.GRU(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "#         self.gru2 = nn.GRU(256, 64, num_layers=1, batch_first=True)\n",
    "#         # self.gru3 = nn.GRU(128, 64, num_layers=1, batch_first=True)\n",
    "#         self.gru4 = nn.GRU(64, 16, num_layers=1, batch_first=True)\n",
    "#         self.fc1 = nn.Linear(16, num_classes)\n",
    "#         # self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.gru1(x)\n",
    "#         out, _ = self.gru2(out)\n",
    "#         # out, _ = self.gru3(out)\n",
    "#         out, _ = self.gru4(out)\n",
    "#         out = self.fc1(out)   \n",
    "#         # out = self.fc2(out)\n",
    "#         out = F.softmax(out, dim = -1)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.lstm3 = nn.LSTM(128, 64, num_layers=1, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(64, 16, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(16, num_classes)\n",
    "        # self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        # out, _ = self.lstm3(out)\n",
    "        out, _ = self.lstm4(out)\n",
    "        out = self.fc1(out)   # Taking only the last time step output\n",
    "        # out = self.fc2(out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErfDataset(Dataset):\n",
    "    def __init__(self, data, emo_index):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.emo_index =  emo_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_embeddings = np.array(self.data[index][1] , dtype='float32')\n",
    "        emotion_sequence = self.data[index][2]\n",
    "        emotion_labels = [self.emo_index[emotion] for emotion in emotion_sequence]\n",
    "        return torch.tensor(sentence_embeddings, dtype= torch.float32), torch.tensor(emotion_labels, dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ErfDataset(train_data, emotion_ids)\n",
    "val_dataset = ErfDataset(val_data, emotion_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, model, optimizer, criterion, device, num_epochs = 30, bs = 32):\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = bs, shuffle = True)\n",
    "    val_dataloader =  DataLoader(val_dataset, batch_size = bs, shuffle = False)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        total_train_loss = 0\n",
    "        all_train_predictions = []\n",
    "        all_train_targets = []\n",
    "\n",
    "        for batch_idx, (inputs, emotions) in enumerate(train_dataloader):\n",
    "            inputs, emotions = inputs.to(device), emotions.to(device)\n",
    "            # print(f\"inputs shape - {inputs.shape}\")\n",
    "            # print(f\"emotions shape - {emotions.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # print(f\"outputs shape - {outputs.shape}\")\n",
    "            loss = 0\n",
    "    \n",
    "            # Iterate over time steps\n",
    "            for i in range(outputs.size(1)):\n",
    "                # print(outputs[:, i, :].shape)\n",
    "                # print(emotions[:, i].long().shape)\n",
    "                loss += criterion(outputs[:, i, :], emotions[:, i].long())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            predictions = outputs.argmax(dim=2).view(-1).cpu().numpy()\n",
    "            targets = emotions.view(-1).cpu().numpy()\n",
    "\n",
    "            all_train_predictions.extend(predictions)\n",
    "            all_train_targets.extend(targets)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        train_macro_f1 = f1_score(all_train_targets, all_train_predictions, average='weighted')\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Training Macro F1-Score: {train_macro_f1}\")\n",
    "\n",
    "        model.eval()  \n",
    "        total_val_loss = 0\n",
    "        all_val_predictions = []\n",
    "        all_val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_emotions in val_dataloader:\n",
    "                val_inputs, val_emotions = val_inputs.to(device), val_emotions.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = 0\n",
    "        \n",
    "                # Iterate over time steps\n",
    "                for i in range(val_outputs.size(1)):\n",
    "                    # Compute loss at each time step\n",
    "                    loss += criterion(val_outputs[:, i, :], val_emotions[:, i].long())\n",
    "\n",
    "                # loss = criterion(val_outputs.view(-1, val_outputs.size(2)), val_emotions.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Convert predictions and targets to numpy arrays\n",
    "                predictions = val_outputs.argmax(dim=2).view(-1).cpu().numpy()\n",
    "                targets = val_emotions.view(-1).cpu().numpy()\n",
    "\n",
    "                all_val_predictions.extend(predictions)\n",
    "                all_val_targets.extend(targets)\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            val_macro_f1 = f1_score(all_val_targets, all_val_predictions, average='weighted')\n",
    "            val_f1_scores.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1},  Validation Loss: {avg_val_loss}, Validation Macro F1-Score: {val_macro_f1}\")\n",
    "\n",
    "\n",
    "    # plot_results(train_losses, val_losses, train_f1_scores, val_f1_scores)\n",
    "    return train_losses, train_f1_scores, val_losses, val_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# class_counts = np.array([23702, 1466, 911, 1021, 576, 229, 225])\n",
    "# class_counts = [83634, 6552, 4188, 4844, 2806, 1177, 1049]\n",
    "# class_counts_tensor = torch.tensor(class_counts, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 38.91980886095353, Training Macro F1-Score: 0.7138093076725853\n",
      "Epoch 1,  Validation Loss: 35.55112075805664, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 2, Training Loss: 35.020241133129325, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 2,  Validation Loss: 34.58672728905311, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 3, Training Loss: 34.51648196737275, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 3,  Validation Loss: 34.325622705312874, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 4, Training Loss: 34.335500731723, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 4,  Validation Loss: 34.2107472052941, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 5, Training Loss: 34.26024016169191, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 5,  Validation Loss: 34.148406542264496, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 6, Training Loss: 34.19851894233063, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 6,  Validation Loss: 34.10882201561561, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 7, Training Loss: 34.17496659373509, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 7,  Validation Loss: 34.08291303194486, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 8, Training Loss: 34.152947666080856, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 8,  Validation Loss: 34.060544087336616, Validation Macro F1-Score: 0.7191713389412965\n",
      "Epoch 9, Training Loss: 34.12592982517854, Training Macro F1-Score: 0.7142164871666569\n",
      "Epoch 9,  Validation Loss: 34.04448362497183, Validation Macro F1-Score: 0.7194686082321907\n",
      "Epoch 10, Training Loss: 34.11108546948615, Training Macro F1-Score: 0.7169994048851366\n",
      "Epoch 10,  Validation Loss: 34.03393877469576, Validation Macro F1-Score: 0.7202865168929237\n",
      "Epoch 11, Training Loss: 34.01777549918371, Training Macro F1-Score: 0.7275192402913463\n",
      "Epoch 11,  Validation Loss: 33.71895012488732, Validation Macro F1-Score: 0.7618446266911174\n",
      "Epoch 12, Training Loss: 33.69320707830764, Training Macro F1-Score: 0.7679578057007268\n",
      "Epoch 12,  Validation Loss: 33.54475006690392, Validation Macro F1-Score: 0.7767588454686852\n",
      "Epoch 13, Training Loss: 33.569330055295055, Training Macro F1-Score: 0.7734836065432428\n",
      "Epoch 13,  Validation Loss: 33.55345373887282, Validation Macro F1-Score: 0.7676428257462159\n",
      "Epoch 14, Training Loss: 33.492258202938636, Training Macro F1-Score: 0.775727734757149\n",
      "Epoch 14,  Validation Loss: 33.4431633582482, Validation Macro F1-Score: 0.7749048054193577\n",
      "Epoch 15, Training Loss: 33.45985319414211, Training Macro F1-Score: 0.7765653408444141\n",
      "Epoch 15,  Validation Loss: 33.37708106407752, Validation Macro F1-Score: 0.7805460660656643\n",
      "Epoch 16, Training Loss: 33.42215947158464, Training Macro F1-Score: 0.7776674662258375\n",
      "Epoch 16,  Validation Loss: 33.4101081261268, Validation Macro F1-Score: 0.7745351839369679\n",
      "Epoch 17, Training Loss: 33.40207471192338, Training Macro F1-Score: 0.7781269319303744\n",
      "Epoch 17,  Validation Loss: 33.47169245206393, Validation Macro F1-Score: 0.7839120311752709\n",
      "Epoch 18, Training Loss: 33.4250599518987, Training Macro F1-Score: 0.7762987399878508\n",
      "Epoch 18,  Validation Loss: 33.340945170475884, Validation Macro F1-Score: 0.7859375281200748\n",
      "Epoch 19, Training Loss: 33.367640342421204, Training Macro F1-Score: 0.7798899224290061\n",
      "Epoch 19,  Validation Loss: 33.312314106867866, Validation Macro F1-Score: 0.7829068622399835\n",
      "Epoch 20, Training Loss: 33.34121961812026, Training Macro F1-Score: 0.7798341150140816\n",
      "Epoch 20,  Validation Loss: 33.3034249819242, Validation Macro F1-Score: 0.7826443849473493\n",
      "Epoch 21, Training Loss: 33.33470724557192, Training Macro F1-Score: 0.780051566550204\n",
      "Epoch 21,  Validation Loss: 33.281165049626274, Validation Macro F1-Score: 0.7808817561698949\n",
      "Epoch 22, Training Loss: 33.31130841670146, Training Macro F1-Score: 0.7808358289199364\n",
      "Epoch 22,  Validation Loss: 33.28663092393141, Validation Macro F1-Score: 0.7809684182401471\n",
      "Epoch 23, Training Loss: 33.302035819483166, Training Macro F1-Score: 0.7812592196677268\n",
      "Epoch 23,  Validation Loss: 33.28735703688402, Validation Macro F1-Score: 0.7801647871385367\n",
      "Epoch 24, Training Loss: 33.291109885878235, Training Macro F1-Score: 0.7817280036122023\n",
      "Epoch 24,  Validation Loss: 33.22809732877291, Validation Macro F1-Score: 0.786332338383399\n",
      "Epoch 25, Training Loss: 33.28491146873882, Training Macro F1-Score: 0.7817910375637692\n",
      "Epoch 25,  Validation Loss: 33.22545256981483, Validation Macro F1-Score: 0.7834944494730617\n",
      "Epoch 26, Training Loss: 33.26716057580846, Training Macro F1-Score: 0.7819120316126154\n",
      "Epoch 26,  Validation Loss: 33.20551270705003, Validation Macro F1-Score: 0.7861225604515696\n",
      "Epoch 27, Training Loss: 33.2334796672559, Training Macro F1-Score: 0.7836632629114342\n",
      "Epoch 27,  Validation Loss: 33.183928122887245, Validation Macro F1-Score: 0.7860171517137473\n",
      "Epoch 28, Training Loss: 33.23181280470986, Training Macro F1-Score: 0.7830269085490766\n",
      "Epoch 28,  Validation Loss: 33.300323486328125, Validation Macro F1-Score: 0.7879911161790224\n",
      "Epoch 29, Training Loss: 33.23876097002102, Training Macro F1-Score: 0.7824248761613197\n",
      "Epoch 29,  Validation Loss: 33.225466361412636, Validation Macro F1-Score: 0.7890438831946169\n",
      "Epoch 30, Training Loss: 33.20181917962227, Training Macro F1-Score: 0.7837650581475838\n",
      "Epoch 30,  Validation Loss: 33.16488999586839, Validation Macro F1-Score: 0.785948318601342\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(768, 7).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss(weight = class_counts_tensor)\n",
    "\n",
    "train_losses, train_f1_scores, val_losses, val_f1_scores = train_model(train_dataset, val_dataset, model = model, num_epochs = 30, optimizer=optimizer, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_results(train_losses, val_losses, train_f1_scores, val_f1_scores):\n",
    "#     epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "\n",
    "#     # Plotting Losses\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs, train_losses, label='Training Loss')\n",
    "#     plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "#     plt.title('Training and Validation Losses')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plotting Macro F1-Scores\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs, train_f1_scores, label='Training Macro F1-Score')\n",
    "#     plt.plot(epochs, val_f1_scores, label='Validation Macro F1-Score')\n",
    "#     plt.title('Training and Validation Macro F1-Scores')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Macro F1-Score')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_test_results(test_loss, test_macro_f1):\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     labels = ['Test Loss', 'Test Macro F1-Score']\n",
    "#     values = [test_loss, test_macro_f1]\n",
    "\n",
    "#     plt.bar(labels, values, color=['blue', 'green'])\n",
    "#     plt.title('Test Results')\n",
    "#     plt.ylabel('Values')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model(task, embedding_type, model, criterion, device, batch_size = 1):\n",
    "#     test_dataloader = None\n",
    "#     if task == 1:\n",
    "#         test_dataloader =  DataLoader(Task_data(task1_test_data, bio_mapping_task1, embedding_type), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     elif task == 2:\n",
    "#         test_dataloader =  DataLoader(Task_data(task2_test_data, bio_mapping_task2, embedding_type), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     total_test_loss = 0\n",
    "#     all_test_predictions = []\n",
    "#     all_test_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for test_inputs, test_targets in test_dataloader:\n",
    "#             test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "#             test_outputs = model(test_inputs)\n",
    "\n",
    "#             loss = 0\n",
    "#             for i in range(test_outputs.size(1)):  # Iterate over time steps\n",
    "#                 loss += criterion(test_outputs[:, i, :], test_targets[:, i])  \n",
    "\n",
    "#             total_test_loss += loss.item()\n",
    "\n",
    "#             all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "#             all_test_targets.extend(test_targets.view(-1).cpu().numpy())\n",
    "\n",
    "#         avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "#         test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='macro')\n",
    "#     print(f'Test Loss: {avg_test_loss}, Test Macro F1-Score: {test_macro_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
