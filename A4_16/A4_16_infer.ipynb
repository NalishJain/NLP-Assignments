{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/nalishjain/Acad Sem 6/NLP-Assignments/A4_16/val_dict.pkl', 'rb') as f:\n",
    "    val_data =  pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "with open('/Users/nalishjain/Acad Sem 6/NLP-Assignments/A4_16/val_dict_speakers.pkl', 'rb') as f:\n",
    "    val_speaker_data =  pickle.load(f, encoding='latin1')\n",
    "    \n",
    "emotion_ids = {'neutral' : 0, 'joy' : 1, 'anger' : 2, 'surprise' : 3, 'sadness' : 4, 'fear' : 5, 'disgust' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan():\n",
    "    # max_length = 25\n",
    "    # pad_word = 'PAD'\n",
    "    # pad_emotion = 'neutral'\n",
    "    data = [val_data, val_speaker_data]\n",
    "\n",
    "    for task_data in data:\n",
    "        remove_keys = []\n",
    "        for key in task_data:\n",
    "            # checking_nan\n",
    "            for step in range(len(task_data[key][3])):\n",
    "                if task_data[key][3][step] is None:\n",
    "                    remove_keys.append(key)\n",
    "\n",
    "        \n",
    "        for key in remove_keys:\n",
    "            if key in task_data:\n",
    "                del task_data[key]  \n",
    "    # print(dict_)\n",
    "\n",
    "remove_nan()\n",
    "val_data = {new_key: val_data[old_key] for new_key, (old_key, _) in enumerate(val_data.items())}\n",
    "val_speaker_data = {new_key: val_speaker_data[old_key] for new_key, (old_key, _) in enumerate(val_speaker_data.items())}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.lstm3 = nn.LSTM(128, 64, num_layers=1, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(64, 16, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(16, num_classes)\n",
    "        # self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        # out, _ = self.lstm3(out)\n",
    "        out, _ = self.lstm4(out)\n",
    "        out = self.fc1(out)   # Taking only the last time step output\n",
    "        # out = self.fc2(out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "# M2\n",
    "class GRUModel_m2(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(GRUModel_m2, self).__init__()\n",
    "        self.gru1 = nn.GRU(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.gru3 = nn.GRU(128, 64, num_layers=1, batch_first=True)\n",
    "        self.gru4 = nn.GRU(64, 16, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(16, num_classes)\n",
    "        # self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru1(x)\n",
    "        out, _ = self.gru2(out)\n",
    "        # out, _ = self.gru3(out)\n",
    "        out, _ = self.gru4(out)\n",
    "        out = self.fc1(out)   \n",
    "        # out = self.fc2(out)\n",
    "        out = F.softmax(out, dim = -1)\n",
    "        return out\n",
    "    \n",
    "# M3\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.gru3 = nn.GRU(128, 64, num_layers=1, batch_first=True)\n",
    "        self.gru4 = nn.GRU(64, 16, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(16, 4)\n",
    "        self.fc2 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru1(x)\n",
    "        out, _ = self.gru2(out)\n",
    "        # out, _ = self.gru3(out)\n",
    "        out, _ = self.gru4(out)\n",
    "        out = self.fc1(out)   \n",
    "        out = self.fc2(out)      \n",
    "        return out\n",
    "    \n",
    "# M4\n",
    "class GRUModel_emotions(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_size):\n",
    "        super(GRUModel_emotions, self).__init__()\n",
    "        self.gru1 = nn.GRU(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.gru3 = nn.GRU(128, 64, num_layers=1, batch_first=True)\n",
    "        self.gru4 = nn.GRU(64, 32, num_layers=1, batch_first=True)\n",
    "        self.one_hot_projection = nn.Linear(7, 16)\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x, emotions):\n",
    "        out, _ = self.gru1(x)\n",
    "        out, _ = self.gru2(out)\n",
    "        # out, _ = self.gru3(out)\n",
    "        out, _ = self.gru4(out)\n",
    "        out = self.fc1(out) \n",
    "        out += self.one_hot_projection(emotions.float()) #Fusing emotions\n",
    "        out = self.fc2(out)      \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErfDataset(Dataset):\n",
    "    def __init__(self, data, emo_index):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.emo_index =  emo_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_embeddings = np.array(self.data[index][1] , dtype='float32')\n",
    "        emotion_sequence = self.data[index][2]\n",
    "        # print(emotion_sequence)\n",
    "        emotion_labels = [self.emo_index[emotion] for emotion in emotion_sequence]\n",
    "        output_labels = np.array(self.data[index][3], dtype='float32')\n",
    "        return torch.tensor(sentence_embeddings, dtype= torch.float32), torch.tensor(emotion_labels), torch.tensor(output_labels, dtype= torch.float32)\n",
    "    \n",
    "class ErcDataset(Dataset):\n",
    "    def __init__(self, data, emo_index):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.emo_index =  emo_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_embeddings = np.array(self.data[index][1] , dtype='float32')\n",
    "        emotion_sequence = self.data[index][2]\n",
    "        emotion_labels = [self.emo_index[emotion] for emotion in emotion_sequence]\n",
    "        return torch.tensor(sentence_embeddings, dtype= torch.float32), torch.tensor(emotion_labels, dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_erf = ErfDataset(val_data, emotion_ids)\n",
    "val_dataset_erc = ErcDataset(val_data, emotion_ids)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Add test dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_1(test_dataset, model, device):\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_emotions in test_dataloader:\n",
    "            test_inputs, test_emotions = test_inputs.to(device), test_emotions.to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_emotions.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "    print(f'Test Loss: {avg_test_loss}, Test Weighted F1-Score: {test_macro_f1}')\n",
    "\n",
    "def test_model_2(test_dataset, model, device):\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_emotions in test_dataloader:\n",
    "            test_inputs, test_emotions = test_inputs.to(device), test_emotions.to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_emotions.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "    print(f'Test Loss: {avg_test_loss}, Test Weighted F1-Score: {test_macro_f1}')\n",
    "\n",
    "def test_model_3(test_dataset, model, device):\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_emotions, test_targets in test_dataloader:\n",
    "            # one_hot_emotions = F.one_hot(test_emotions, num_classes=7)\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_targets.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "    print(f'Test Loss: {avg_test_loss}, Test Weighted F1-Score: {test_macro_f1}')\n",
    "\n",
    "def test_model_4(test_dataset, model, device):\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_emotions, test_targets in test_dataloader:\n",
    "            one_hot_emotions = F.one_hot(test_emotions, num_classes=7)\n",
    "            test_inputs, one_hot_emotions, test_targets = test_inputs.to(device), one_hot_emotions.to(device), test_targets.to(device)\n",
    "            test_outputs = model(test_inputs, one_hot_emotions)\n",
    "\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_targets.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "    print(f'Test Loss: {avg_test_loss}, Test Weighted F1-Score: {test_macro_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0, Test Weighted F1-Score: 0.6203351842352914\n"
     ]
    }
   ],
   "source": [
    "loaded_model = LSTMModel(768, 7).to(device) \n",
    "loaded_model.load_state_dict(torch.load('model_m1_dict.pt'))\n",
    "test_model_1(val_dataset_erc, loaded_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0, Test Weighted F1-Score: 0.7412858709165464\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GRUModel_m2(768, 7).to(device) \n",
    "loaded_model.load_state_dict(torch.load('model_m2_dict.pt'))\n",
    "test_model_2(val_dataset_erc, loaded_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0, Test Weighted F1-Score: 0.30591673359002275\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GRUModel(768, 2).to(device) \n",
    "loaded_model.load_state_dict(torch.load('model_3_dict.pt'))\n",
    "test_model_3(val_dataset_erf, loaded_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0, Test Weighted F1-Score: 0.6880068166184801\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GRUModel_emotions(768, 2).to(device) \n",
    "loaded_model.load_state_dict(torch.load('model_4_dict.pt'))\n",
    "test_model_4(val_dataset_erf, loaded_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLIP DETECTION IN ERC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_speakers = set()\n",
    "for i in range(len(val_speaker_data)):\n",
    "    for s in val_speaker_data[i][4]:\n",
    "        unique_speakers.add(s) \n",
    "\n",
    "speaker_ids = {speaker: i for i, speaker in enumerate(unique_speakers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERCSpeakerDataset(Dataset):\n",
    "    def __init__(self, data, emo_index, speaker_to_index):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.emo_index =  emo_index\n",
    "        self.speaker_to_idx = speaker_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_embeddings = np.array(self.data[index][1] , dtype='float32')\n",
    "        emotion_sequence = self.data[index][2]\n",
    "        speaker_sequence = self.data[index][4]\n",
    "        emotion_labels = [self.emo_index[emotion] for emotion in emotion_sequence]\n",
    "        speaker_labels = [self.speaker_to_idx[speaker] for speaker in speaker_sequence]\n",
    "        return torch.tensor(sentence_embeddings, dtype= torch.float32), torch.tensor(emotion_labels, dtype= torch.float32), torch.tensor(speaker_labels, dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_speaker_dataset = ERCSpeakerDataset(val_speaker_data, emotion_ids, speaker_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_flips(test_dataset, model, device):\n",
    "\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        true_f = 0\n",
    "        total_f = 0\n",
    "\n",
    "        for test_inputs, test_emotions, test_speakers in test_dataloader:\n",
    "\n",
    "            speaker_true_emotion = {i: 9 for i in range(245)}\n",
    "            speaker_pred_emotion = {i: 9 for i in range(245)}\n",
    "\n",
    "            test_inputs, test_emotions, test_speakers = test_inputs.to(device), test_emotions.to(device), test_speakers.to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_emotions.view(-1).cpu().numpy())\n",
    "\n",
    "            predictions = test_outputs.argmax(dim=2).view(-1).cpu().numpy()\n",
    "            targets = test_emotions.view(-1).long().cpu().numpy()\n",
    "            speakers = test_speakers.view(-1).long().cpu().numpy()\n",
    "\n",
    "\n",
    "            total_flips = 0\n",
    "            true_flips = 0\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "\n",
    "                if speaker_true_emotion[speakers[i]] == 9 and speaker_pred_emotion[speakers[i]] == 9:\n",
    "                    speaker_true_emotion[speakers[i]] = targets[i]\n",
    "                    speaker_pred_emotion[speakers[i]] = predictions[i]\n",
    "\n",
    "                elif speaker_true_emotion[speakers[i]] != targets[i]:\n",
    "                    total_flips += 1\n",
    "                    if predictions[i] == targets[i] and speaker_pred_emotion[speakers[i]] != predictions[i] and speaker_pred_emotion[speakers[i]] == speaker_true_emotion[speakers[i]]:\n",
    "                        true_flips += 1\n",
    "            \n",
    "                    speaker_pred_emotion[speakers[i]] = predictions[i]  \n",
    "                    speaker_true_emotion[speakers[i]] = targets[i]           \n",
    "            \n",
    "                else:\n",
    "                    speaker_pred_emotion[speakers[i]] = predictions[i]\n",
    "\n",
    "\n",
    "            true_f += true_flips\n",
    "            total_f += total_flips\n",
    "           \n",
    "            \n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "\n",
    "    print(f'Test Macro F1-Score: {test_macro_f1}')\n",
    "    print(f\"Number of flips detected by model - {true_f}\")\n",
    "    print(f\"Total number of flips in the data - {total_f}\")\n",
    "    print(f\"Percentage of flips detected - {(true_f/total_f * 100):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro F1-Score: 0.6203351842352914\n",
      "Number of flips detected by model - 1103\n",
      "Total number of flips in the data - 3132\n",
      "Percentage of flips detected - 35.22 %\n"
     ]
    }
   ],
   "source": [
    "loaded_model_lstm = LSTMModel(768, 7).to(device) \n",
    "loaded_model_lstm.load_state_dict(torch.load('model_m1_dict.pt'))\n",
    "test_model_flips(val_speaker_dataset, loaded_model_lstm, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro F1-Score: 0.7412858709165464\n",
      "Number of flips detected by model - 1609\n",
      "Total number of flips in the data - 3132\n",
      "Percentage of flips detected - 51.37 %\n"
     ]
    }
   ],
   "source": [
    "loaded_model_gru = GRUModel_m2(768, 7).to(device) \n",
    "loaded_model_gru.load_state_dict(torch.load('model_m2_dict.pt'))\n",
    "test_model_flips(val_speaker_dataset, loaded_model_gru, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
