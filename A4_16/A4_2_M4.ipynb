{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/nalishjain/Acad Sem 6/NLP-Assignments/A4_16/train_dict.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "with open('/Users/nalishjain/Acad Sem 6/NLP-Assignments/A4_16/val_dict.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "with open('/Users/nalishjain/Acad Sem 6/NLP-Assignments/A4_16/val_dict.pkl', 'rb') as f:\n",
    "    val_without_pad =  pickle.load(f, encoding='latin1')\n",
    "\n",
    "emotion_ids = {'neutral' : 0, 'joy' : 1, 'anger' : 2, 'surprise' : 3, 'sadness' : 4, 'fear' : 5, 'disgust' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surprise', 'fear', 'surprise', 'sadness', 'disgust']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n",
      "628\n",
      "676\n",
      "1240\n",
      "1308\n",
      "1642\n",
      "3257\n",
      "3368\n",
      "3764\n",
      "4177\n",
      "175\n",
      "378\n",
      "504\n",
      "528\n",
      "175\n",
      "378\n",
      "504\n",
      "528\n"
     ]
    }
   ],
   "source": [
    "def pad_data():\n",
    "    max_length = 25\n",
    "    pad_word = 'PAD'\n",
    "    pad_emotion = 'neutral'\n",
    "    data = [train_data, val_data]\n",
    "    dict_ = {}\n",
    "\n",
    "    for task_data in data:\n",
    "        remove_keys = []\n",
    "        for key in task_data:\n",
    "            if len(task_data[key][0]) not in dict_:\n",
    "                dict_[len(task_data[key][0])] = 0\n",
    "            dict_[len(task_data[key][0])] += 1\n",
    "            # checking_nan\n",
    "            for step in range(len(task_data[key][3])):\n",
    "                if task_data[key][3][step] is None:\n",
    "                    remove_keys.append(key)\n",
    "                    print(key)\n",
    "            for step in range(len(task_data[key][0]), max_length):\n",
    "                task_data[key][0].append(pad_word)\n",
    "                task_data[key][1].append(np.zeros(768))\n",
    "                task_data[key][2].append(pad_emotion)\n",
    "                task_data[key][3].append(0)\n",
    "        \n",
    "        for key in remove_keys:\n",
    "            if key in task_data:\n",
    "                del task_data[key] \n",
    "\n",
    "def remove_nan():\n",
    "    # max_length = 25\n",
    "    # pad_word = 'PAD'\n",
    "    # pad_emotion = 'neutral'\n",
    "    data = [val_without_pad]\n",
    "    dict_ = {}\n",
    "\n",
    "    for task_data in data:\n",
    "        remove_keys = []\n",
    "        for key in task_data:\n",
    "            if len(task_data[key][0]) not in dict_:\n",
    "                dict_[len(task_data[key][0])] = 0\n",
    "            dict_[len(task_data[key][0])] += 1\n",
    "            # checking_nan\n",
    "            for step in range(len(task_data[key][3])):\n",
    "                if task_data[key][3][step] is None:\n",
    "                    remove_keys.append(key)\n",
    "                    print(key)\n",
    "            # for step in range(len(task_data[key][0]), max_length):\n",
    "            #     task_data[key][0].append(pad_word)\n",
    "            #     task_data[key][1].append(np.zeros(768))\n",
    "            #     task_data[key][2].append(pad_emotion)\n",
    "            #     task_data[key][3].append(0)\n",
    "        \n",
    "        for key in remove_keys:\n",
    "            if key in task_data:\n",
    "                del task_data[key]  \n",
    "    # print(dict_)\n",
    "\n",
    "pad_data()\n",
    "remove_nan()\n",
    "train_data = {new_key: train_data[old_key] for new_key, (old_key, _) in enumerate(train_data.items())}\n",
    "val_data = {new_key: val_data[old_key] for new_key, (old_key, _) in enumerate(val_data.items())}\n",
    "val_without_pad = {new_key: val_without_pad[old_key] for new_key, (old_key, _) in enumerate(val_without_pad.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4170\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class GRUModel_emotions(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_size):\n",
    "        super(GRUModel_emotions, self).__init__()\n",
    "        self.gru1 = nn.GRU(embedding_dim, 256, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 64, num_layers=1, batch_first=True)\n",
    "        # self.gru3 = nn.GRU(128, 64, num_layers=1, batch_first=True)\n",
    "        self.gru4 = nn.GRU(64, 32, num_layers=1, batch_first=True)\n",
    "        self.one_hot_projection = nn.Linear(7, 16)\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x, emotions):\n",
    "        out, _ = self.gru1(x)\n",
    "        out, _ = self.gru2(out)\n",
    "        # out, _ = self.gru3(out)\n",
    "        out, _ = self.gru4(out)\n",
    "        out = self.fc1(out) \n",
    "        out += self.one_hot_projection(emotions.float()) #Fusing emotions\n",
    "        out = self.fc2(out)      \n",
    "        return out\n",
    "\n",
    "class ErfDataset(Dataset):\n",
    "    def __init__(self, data, emo_index):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.emo_index =  emo_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_embeddings = np.array(self.data[index][1] , dtype='float32')\n",
    "        emotion_sequence = self.data[index][2]\n",
    "        # print(emotion_sequence)\n",
    "        emotion_labels = [self.emo_index[emotion] for emotion in emotion_sequence]\n",
    "        output_labels = np.array(self.data[index][3], dtype='float32')\n",
    "        return torch.tensor(sentence_embeddings, dtype= torch.float32), torch.tensor(emotion_labels), torch.tensor(output_labels, dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ErfDataset(train_data, emotion_ids)\n",
    "val_dataset = ErfDataset(val_data, emotion_ids)\n",
    "val_without_pad_dataset = ErfDataset(val_without_pad, emotion_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_emotions = F.one_hot(train_dataset[0][1], num_classes=7)\n",
    "one_hot_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, model, optimizer, criterion, device,  num_epochs = 30, batch_size = 32):\n",
    "    test_dataloader = None\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader =  DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(len(train_dataloader))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        all_train_predictions = []\n",
    "        all_train_targets = []\n",
    "\n",
    "        for batch_idx, (inputs, emotions, targets) in enumerate(train_dataloader):\n",
    "            one_hot_emotions = F.one_hot(emotions, num_classes=7)\n",
    "            # print(batch_idx, \"\\\\\", len(train_dataloader))\n",
    "            inputs, one_hot_emotions, targets = inputs.to(device), one_hot_emotions.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, one_hot_emotions)\n",
    "            \n",
    "            loss = 0\n",
    "\n",
    "            for i in range(outputs.size(1)):  # Iterate over time steps\n",
    "                # print(type(outputs[:, i, :]))\n",
    "                loss += criterion(outputs[:, i, :], targets[:, i].long())  \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            all_train_predictions.extend(outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_train_targets.extend(targets.view(-1).cpu().numpy())\n",
    "\n",
    "        print(np.sum(np.array(all_train_predictions)), np.sum(np.array(all_train_targets)), len(all_train_targets))\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # return all_train_targets\n",
    "        train_macro_f1 = f1_score(all_train_targets, all_train_predictions, average='weighted')\n",
    "        train_f1_scores.append(train_macro_f1)\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Training Macro F1-Score: {train_macro_f1}\")\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        all_val_predictions = []\n",
    "        all_val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_emotions, val_targets in val_dataloader:\n",
    "                one_hot_emotions = F.one_hot(val_emotions, num_classes=7)\n",
    "                val_inputs, one_hot_emotions, val_targets = val_inputs.to(device), one_hot_emotions.to(device), val_targets.to(device)\n",
    "                val_outputs = model(val_inputs, one_hot_emotions)\n",
    "\n",
    "                loss = 0\n",
    "                for i in range(val_outputs.size(1)):  # Iterate over time steps\n",
    "                    loss += criterion(val_outputs[:, i, :], val_targets[:, i].long())  \n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                all_val_predictions.extend(val_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "                all_val_targets.extend(val_targets.view(-1).cpu().numpy())\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            val_macro_f1 = f1_score(all_val_targets, all_val_predictions, average='weighted')\n",
    "            val_f1_scores.append(val_macro_f1)\n",
    "        print(f\"Epoch {epoch + 1},  Validation Loss: {avg_val_loss}, Validation Macro F1-Score: {val_macro_f1}\")\n",
    "    plot_results(train_losses, val_losses, train_f1_scores, val_f1_scores)\n",
    "\n",
    "    return train_losses, train_f1_scores, val_losses, val_f1_scores\n",
    "\n",
    "def plot_results(train_losses, val_losses, train_f1_scores, val_f1_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plotting Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Macro F1-Scores\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(epochs, train_f1_scores, label='Training Macro F1-Score')\n",
    "    # plt.plot(epochs, val_f1_scores, label='Validation Macro F1-Score')\n",
    "    # plt.title('Training and Validation Macro F1-Scores')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Macro F1-Score')\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_model(test_dataset, model, device):\n",
    "    test_dataloader = None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    total_test_loss = 0\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_emotions, test_targets in test_dataloader:\n",
    "            one_hot_emotions = F.one_hot(test_emotions, num_classes=7)\n",
    "            test_inputs, one_hot_emotions, test_targets = test_inputs.to(device), one_hot_emotions.to(device), test_targets.to(device)\n",
    "            test_outputs = model(test_inputs, one_hot_emotions)\n",
    "\n",
    "            all_test_predictions.extend(test_outputs.argmax(dim=2).view(-1).cpu().numpy())\n",
    "            all_test_targets.extend(test_targets.view(-1).cpu().numpy())\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        test_macro_f1 = f1_score(all_test_targets, all_test_predictions, average='weighted')\n",
    "    print(f'Test Loss: {avg_test_loss}, Test Macro F1-Score: {test_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUModel_emotions(768, 2).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.05,6.16]))\n",
    "\n",
    "train_losses, train_f1_scores, val_losses, val_f1_scores = train_model(train_dataset, val_dataset, model = model, num_epochs = 30, optimizer=optimizer, criterion=criterion, batch_size=32, device=device)\n",
    "# torch.save(model.state_dict(), 'model_4_dict.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0, Test Macro F1-Score: 0.6880068166184801\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GRUModel_emotions(768, 2)\n",
    "loaded_model.load_state_dict(torch.load('model_4_dict.pt'))\n",
    "test_model(val_without_pad_dataset, loaded_model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
