{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A man with a hard hat is dancing.</td>\n",
       "      <td>A man wearing a hard hat is dancing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.75</td>\n",
       "      <td>A young child is riding a horse.</td>\n",
       "      <td>A child is riding a horse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A man is feeding a mouse to a snake.</td>\n",
       "      <td>The man is feeding a mouse to the snake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.40</td>\n",
       "      <td>A woman is playing the guitar.</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.75</td>\n",
       "      <td>A woman is playing the flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                             sentence1  \\\n",
       "0   5.00     A man with a hard hat is dancing.   \n",
       "1   4.75      A young child is riding a horse.   \n",
       "2   5.00  A man is feeding a mouse to a snake.   \n",
       "3   2.40        A woman is playing the guitar.   \n",
       "4   2.75         A woman is playing the flute.   \n",
       "\n",
       "                                  sentence2  \n",
       "0      A man wearing a hard hat is dancing.  \n",
       "1                A child is riding a horse.  \n",
       "2  The man is feeding a mouse to the snake.  \n",
       "3                  A man is playing guitar.  \n",
       "4                 A man is playing a flute.  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/anike/Desktop/IIIT D/NLP/Ass3'\n",
    "path = 'c:\\\\Users\\\\Shobhit\\\\Desktop\\\\IIITacad\\\\Sem6\\\\NLP_Assignments\\\\Assignment3'\n",
    "data = pd.read_csv(path + '\\\\train.csv')\n",
    "val_data =pd.read_csv(path +'\\\\dev.csv', sep = \"\\t\")\n",
    "#change column name from setence1 to sentence1\n",
    "val_data = val_data.rename(columns = {'setence1':'sentence1'})\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "sentences1 = data[\"sentence1\"]\n",
    "sentences2 = data[\"sentence2\"]\n",
    "labels = data[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_losses,val_losses):\n",
    "    epochs=range(1,len(train_losses)+1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plotting Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self, model_name, device = \"cpu\"):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def encode(self, sentences1, sentences2):\n",
    "        inputs = self.tokenizer(sentences1, sentences2, return_tensors=\"pt\", max_length = 30, padding=\"max_length\", truncation=True)\n",
    "        inputs.to(self.device)\n",
    "        return inputs    \n",
    "\n",
    "    def add_linear_layer(self, num_labels):\n",
    "        self.model.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "        self.model.num_labels = num_labels\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, sentences1, sentences2):\n",
    "        inputs = self.encode(sentences1, sentences2)\n",
    "        outputs = self.forward(inputs)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "    \n",
    "    def train(self, train_dataloader,val_dataloader, optimizer, loss_fn, epochs):\n",
    "        #train on GPU\n",
    "        train_losses=[]\n",
    "        val_losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            i = 0\n",
    "            self.model.train()  # Set the model to training mode\n",
    "            total_train_loss = 0\n",
    "            all_train_predictions = [] \n",
    "            all_train_targets = []\n",
    "            for data in train_dataloader:\n",
    "                i += 1\n",
    "                print(\"Training Epoch: \", epoch, \" Batch: \", i)\n",
    "                optimizer.zero_grad()\n",
    "                inputs = data[0].to(self.device)\n",
    "                attention_mask = data[1].to(self.device)\n",
    "                labels = data[2].to(self.device)\n",
    "                outputs = self.model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss+=loss.item()\n",
    "                \n",
    "                all_train_predictions.extend(outputs[\"logits\"].argmax(dim=1).view(-1).cpu().numpy())\n",
    "                all_train_targets.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss}\")\n",
    "            self.model.eval()  # Set the model to evaluation mode\n",
    "            total_val_loss = 0\n",
    "            all_val_predictions = []\n",
    "            all_val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_dataloader:\n",
    "                    val_inputs,val_attention_mask,val_labels=val_data\n",
    "                    val_inputs = val_inputs.to(self.device)\n",
    "                    val_attention_mask = val_attention_mask.to(self.device)\n",
    "                    val_labels = val_labels.to(self.device)\n",
    "                    val_outputs = self.model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "                    \n",
    "                    loss=val_outputs.loss\n",
    "\n",
    "                    total_val_loss += loss.item()\n",
    "                    \n",
    "                    all_val_predictions.extend(val_outputs[\"logits\"].argmax(dim=1).view(-1).cpu().numpy())\n",
    "                    all_val_targets.extend(val_labels.view(-1).cpu().numpy())\n",
    "\n",
    "                avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch {epoch + 1},  Validation Loss: {avg_val_loss}\")\n",
    "        plot_results(train_losses, val_losses)\n",
    "        return train_losses,  val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, model_name):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = str(self.data.iloc[idx]['sentence1'])\n",
    "        sentence2 = str(self.data.iloc[idx]['sentence2'])\n",
    "        label = self.data.iloc[idx]['score']\n",
    "        inputs = self.tokenizer(sentence1, sentence2, return_tensors=\"pt\", max_length = 256, padding=\"max_length\", truncation=True)\n",
    "        return inputs, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def get_encoded_data(self):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "        for i in range(self.__len__()):\n",
    "            input, label = self.__getitem__(i)\n",
    "            input_ids.append(input['input_ids'])\n",
    "            attention_masks.append(input['attention_mask'])\n",
    "            labels.append(label.float()/5.0)\n",
    "        \n",
    "        input_ids = torch.cat(input_ids, dim=0).squeeze(0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0).squeeze(0)\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    def get_dataloader(self, batch_size):\n",
    "        input_ids, attention_masks, labels = self.get_encoded_data()\n",
    "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs1, inputs2 = data  # Modify this based on your test dataset structure\n",
    "            outputs = model.predict(inputs1, inputs2)  # Modify this based on your model's prediction method\n",
    "            predictions = ...  # Process outputs to get predictions\n",
    "            all_predictions.extend(predictions)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(data,model_name)\n",
    "val_dataset=CustomDataset(val_data,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:  0  Batch:  1\n",
      "Training Epoch:  0  Batch:  2\n",
      "Training Epoch:  0  Batch:  3\n",
      "Training Epoch:  0  Batch:  4\n",
      "Training Epoch:  0  Batch:  5\n",
      "Training Epoch:  0  Batch:  6\n",
      "Training Epoch:  0  Batch:  7\n",
      "Training Epoch:  0  Batch:  8\n",
      "Training Epoch:  0  Batch:  9\n",
      "Training Epoch:  0  Batch:  10\n",
      "Training Epoch:  0  Batch:  11\n",
      "Training Epoch:  0  Batch:  12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m Model \u001b[38;5;241m=\u001b[39m model(model_name)\n\u001b[0;32m      2\u001b[0m Model\u001b[38;5;241m.\u001b[39madd_linear_layer(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Modify this with your actual test data file path\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_data_path)\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mmodel.train\u001b[1;34m(self, train_dataloader, val_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 51\u001b[0m total_train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m all_train_predictions\u001b[38;5;241m.\u001b[39mextend(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     54\u001b[0m all_train_targets\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Model = model(model_name)\n",
    "Model.add_linear_layer(1)\n",
    "Model.train(dataset.get_dataloader(8),val_dataset.get_dataloader(8), torch.optim.Adam(Model.model.parameters()), nn.CrossEntropyLoss(), 1)\n",
    "test_data_path = 'test_data.csv'  # Modify this with your actual test data file path\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "# Create DataLoader for test data\n",
    "test_dataset = CustomDataset(test_data)  # Modify this based on your test dataset class\n",
    "test_loader = test_dataset.get_dataloader(8)\n",
    "predictions = generate_predictions(model, test_loader)\n",
    "# Create CSV file\n",
    "output_df = pd.DataFrame({'Prediction': predictions})\n",
    "output_df.to_csv('sample_demo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
